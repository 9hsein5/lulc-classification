{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "collapsed_sections": [
    "cZzXC7IHCazM",
    "ET8xcMK0WEeJ",
    "KLhzjCifTcW5"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/climatechange-ai-tutorials/lulc-classification/blob/fix%2Fval-set/land_use_land_cover_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFpgp_rIA_TY"
   },
   "source": [
    "# Land Use and Land Cover Classification using Pytorch\n",
    "**Content Creators**: Isabelle Tingzon and Ankur Mahesh\n",
    "\n",
    "Welcome to CCAI's tutorial on land use and land cover (LULC) classification using Pytorch!\n",
    "\n",
    "In this two-part tutorial, you will learn how to:\n",
    "- train a deep learning model for image classification using Pytorch\n",
    "- generate land use and land cover maps using Python GIS\n",
    "\n",
    "You can make a copy of this tutorial by File\u2192Save a copy in Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "*   [Overview](#overview)\n",
    "*   [Climate Impact](#climate-impact)\n",
    "*   [Target Audience](#target-audience)\n",
    "*   [Background & Prerequisites](#background-and-prereqs)\n",
    "*   [Software Requirements](#software-requirements)\n",
    "*   [Data Description](#data-description)\n",
    "*   [Methodology](#methodology)\n",
    "*   [Results](#results)\n",
    "*   [Exercises](#exercises)\n",
    "*   [References](#references)"
   ],
   "metadata": {
    "id": "gchK901cq3hN"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuv5XES8Ejc_"
   },
   "source": [
    "<a name=\"overview\"></a>\n",
    "# Overview\n",
    "This tutorial covers an introduction to image classification using Pytorch for land use and land cover (LULC) mapping.\n",
    "\n",
    "Specifically, you will learn how to:\n",
    "- classify satellite images into 10 LULC categories using the [EuroSAT dataset](https://arxiv.org/abs/1709.00029)\n",
    "- fine-tune a Resnet-50 CNN model for image classification\n",
    "- save and load trained models in Pytorch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"climate-impact\"></a>\n",
    "# Climate Impact\n",
    "A [report](https://www.wri.org/insights/7-things-know-about-ipccs-special-report-climate-change-and-land) by the World Resources Institute (WRI) states that about 23% of global human-caused GHG emissions come from land uses such as agriculture, forestry, and urban expansion. Land use change such as deforestation and land degradation are among the primary drivers of these emissions. Rapid urbanization leading to an increase in built-up areas as well as a massive loss of terrestrial carbon storage can also result in large carbon emissions.\n",
    "\n",
    "Mapping the extent of land use and land cover categories over time is essential for better environmental monitoring, urban planning and nature protection. For example, monitoring changes in forest cover and identifying drivers of forest loss can be useful for forest conservation and restoration efforts. Assessing the vulnerability of certain land cover types, such as settlements and agricultural land, to certain risks can also be useful for for disaster risk reduction planning as well as long-term climate adaptation efforts.\n",
    "\n",
    "With the increasing availability of earth observation data coupled with recent advanced in computer vision, AI & EO has paved the way for the potential to map land use and land cover at an unprecedented scale. In this tutorial, we will explore the use of Sentinel-2 satellite images and deep learning models in Pytorch to automate LULC mapping.\n",
    "\n",
    "<br>\n",
    "<center><p><p> <img src=\"https://ptes.org/wp-content/uploads/2018/04/iStock-664630460-e1524839082464.jpg\" alt=\"alt\" width=\"50%\"/>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "6yqHcIOjZD2c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"target-audience\"></a>\n",
    "# Target Audience\n",
    "This tutorial is intended for experienced and aspiring data scientists looking for concrete examples of the applications of deep learning in climate change. We expect users to have some background in machine learning, including neural networks. But don't worry if you are new to these topics! We will provide additional resources and external links below for you to further study."
   ],
   "metadata": {
    "id": "xyb28HbsrcLJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"background-and-prereqs\"></a>\n",
    "## Background and Prerequisites\n",
    "For a refresher on neural networks, feel free to check out the video below by [3Blue1Brown](https://www.youtube.com/c/3blue1brown).\n",
    "\n",
    "We also highly recommend [Stanford's lecture collection](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) on convolutional neural networks (CNNs) for visual recognition. The deep learning specialization courses at [deeplearning.ai](https://www.deeplearning.ai/courses/) also provide an in-depth introduction to ANNs, CNNs, sequence models, and other deep learning concepts."
   ],
   "metadata": {
    "id": "AaV01KXRrGAe"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "HxshqdcpvvHB",
    "outputId": "a0c300c8-07bc-48e2-a2f4-75d1b17f3322"
   },
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('P28LKWTzrI')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"software-requirements\"></a>\n",
    "# Software Requirements\n",
    "\n",
    "This notebook requires Python >= 3.7. The following libraries are required:\n",
    "*   tqdm\n",
    "*   pandas\n",
    "*   numpy\n",
    "*   matplotlib\n",
    "*   pytorch"
   ],
   "metadata": {
    "id": "PMhFV8gzrqO-"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H08M_-5iEbc1"
   },
   "source": [
    "## Enabling GPU in Google Colab\n",
    "Before we start, you will need access to a GPU.  Fortunately, Google Colab provides free access to computing resources including GPUs. The GPUs currently available in Colab include Nvidia K80s, T4s, P4s and P100s. Unfortunately, there is no way to choose what type of GPU you can connect to in Colab. [See here for information](https://research.google.com/colaboratory/faq.html#gpu-availability).\n",
    "\n",
    "To enable GPU in Google Colab:\n",
    "1. Navigate to Edit\u2192Notebook Settings or Runtime\u2192Change Runtime Type.\n",
    "2. Select GPU from the Hardware Accelerator drop-down."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qw688STi6Z6k"
   },
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchsummary\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ffVUKx7P6RD"
   },
   "source": [
    "## Google Colab GPU\n",
    "Check that the GPU  enabled in your colab notebook by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPzhNed7P2i9",
    "outputId": "5e3c68ed-7b6a-451b-c266-9d73597efa04"
   },
   "source": [
    "# Check is GPU is enabled\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Get specific GPU model\n",
    "if str(device) == \"cuda:0\":\n",
    "  print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL9_-2ilI0qv"
   },
   "source": [
    "## Mount Drive\n",
    "\n",
    "Mounting the drive will allow the Google Colab notebook to load and access files from your Google drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P79X72ZCI2TA",
    "outputId": "7f9db116-7248-47f0-ad3e-853d9912c2e3"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIQAlBV9GbCZ"
   },
   "source": [
    "<a name=\"data-description\"></a>\n",
    "# Data Description\n",
    "\n",
    "In this section, you will learn how to:\n",
    "- Download the EuroSAT dataset into your Google Drive\n",
    "- Generate the train and test sets by splitting the EuroSAT dataset\n",
    "- Visualize a sample of the images and their LULC labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK0AM2bWGxsw"
   },
   "source": [
    "## EuroSAT Dataset\n",
    "The [EuroSAT dataset](https://github.com/phelber/EuroSAT) contains 27,000 labelled 64x64 pixel Sentinel-2 satellite image patches with 10 different LULC categories. Both RGB and multi-spectral (MS) images are available for download. For simplicity, we will focus on RGB image classification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GNyqemwGwr9",
    "outputId": "33367b05-4789-4d58-adf0-83ec5cfcbbf6"
   },
   "source": [
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip -O EuroSAT.zip\n",
    "!unzip -q EuroSAT.zip -d 'EuroSAT/'\n",
    "!rm EuroSAT.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp_i70hQ-ZcX"
   },
   "source": [
    "## Generate Train and Test Sets\n",
    "\n",
    "### Create Custom Dataset Class\n",
    "In Pytorch, the `Dataset` class allows you to define a custom class to load the input and target for a dataset.  We will use this capability to load in our inputs in the form of RGB satellite images along with their corresponding labels. Later we'll learn how to apply necessary image transformations (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SsK-rDJyUFpV"
   },
   "source": [
    "class EuroSAT(data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Apply image transformations\n",
    "        if self.transform:\n",
    "            x = self.transform(dataset[index][0])\n",
    "        else:\n",
    "            x = dataset[index][0]\n",
    "        # Get class label\n",
    "        y = dataset[index][1]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9bbA-_d-Vis"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data augmentation is a  technique that randomly applies image transformations, e.g. crops, horizontal flips, and vertical flips, to the input images during model training. These perturbations reduce the neural network's overfitting to the training dataset, and they allow it to generalize better to the unseen test dataset.\n",
    "<br><br>\n",
    "<center> <br>\n",
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*ttoU2HOnBI8cb9Y2.png\" width=\"400\"/>\n",
    "<br>\n",
    "<font size=2>Image Source: <a href=\"https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010\">https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010</a></a> </font>\n",
    "<br>\n",
    "</center>\n",
    "</center>\n",
    "<br>\n",
    "<font size=2>Image Source: Ahmad, Jamil & Muhammad, Khan & Baik, Sung. (2017). Data augmentation-assisted deep learning of hand-drawn partially colored sketches for visual search. PLOS ONE. 12. e0183838. 10.1371/journal.pone.0183838. </font>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Image Normalization\n",
    "Additionally, in the cell below, the `transforms.Normalize` method normalizes each of the three channels to the given means and standard deviations defined in the `imagenet_mean` and `imagenet_std` variables. ImageNet is a large training dataset of images and labels.  Later in this tutorial, we will be using a model pre-trained on this dataset.  In order to use this pre-trained model for our LULC dataset, we need to ensure that the input dataset is normalized to have the same statistics (mean and standard deviation) as ImageNet.\n",
    "\n",
    "<br>\n",
    "<center> <img src=\"https://cv.gluon.ai/_images/imagenet_banner.jpeg\" width=\"400\"/>\n",
    "<br>\n",
    "<font size=2>Image Source: <a href=\" https://cv.gluon.ai/build/examples_datasets/imagenet.html\">https://cv.gluon.ai/build/examples_datasets/imagenet.html</a></font>\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "Existing research has found that using models pretrained on massive datasets, such as ImageNet, improves accuracy when applying these neural networks to new datasets.  Pre-trained models serve as excellent generic feature extractors.  [Please read here for more information](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "osNVdWjtIQMK"
   },
   "source": [
    "input_size = 224\n",
    "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-H1Y1rHVz8t"
   },
   "source": [
    "### Load EuroSAT Dataset\n",
    "Let's start by loading the EuroSAT dataset using torch's `ImageFolder` class.\n",
    "\n",
    "`ImageFolder` is a generic data loader where the images are arranged in this way:\n",
    "\n",
    "```\n",
    "    data\n",
    "    \u2514\u2500\u2500\u2500AnnualCrop\n",
    "    \u2502   \u2502   AnnualCrop_1.jpg\n",
    "    \u2502   \u2502   AnnualCrop_2.jpg\n",
    "    \u2502   \u2502   AnnualCrop_3.jpg\n",
    "    \u2502   \u2502   ...\n",
    "    \u2514\u2500\u2500\u2500Forest\n",
    "    \u2502   \u2502   Forest_1.jpg\n",
    "    \u2502   \u2502   Forest_2.jpg\n",
    "    \u2502   \u2502   Forest_3.jpg\n",
    "    \u2502   \u2502   ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TM7hyGz3Ry8e",
    "outputId": "c5908348-9a70-42e2-8dc8-381c8907e1b6"
   },
   "source": [
    "# Load the dataset\n",
    "data_dir = './EuroSAT/2750/'\n",
    "dataset = datasets.ImageFolder(data_dir)\n",
    "\n",
    "# Get LULC categories\n",
    "class_names = dataset.classes\n",
    "print(\"Class names: {}\".format(class_names))\n",
    "print(\"Total number of classes: {}\".format(len(class_names)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY8oTO459LXo"
   },
   "source": [
    "### Split into Train, Validation, and Test Sets\n",
    "Here, we split the dataset into a train set and test set. The training set will be 70% of the Eurosat dataset, randomly selected. We set aside 15% of the dataset as our validation set and the remaining 15% as our test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6_Jm_Vt89-L",
    "outputId": "686e2ed0-0da9-421a-9abe-02a1d39a2ecd"
   },
   "source": [
    "# Apply different transformations to the training and test sets\n",
    "train_data = EuroSAT(dataset, train_transform)\n",
    "val_data = EuroSAT(dataset, val_transform)\n",
    "test_data = EuroSAT(dataset, test_transform)\n",
    "\n",
    "# Randomly split the dataset into 70% train / 15% val / 15% test\n",
    "# by subsetting the transformed train and test datasets\n",
    "train_size = 0.70\n",
    "val_size = 0.15\n",
    "indices = list(range(int(len(dataset))))\n",
    "train_split = int(train_size * len(dataset))\n",
    "val_split = int(val_size * len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_data = data.Subset(train_data, indices=indices[:train_split])\n",
    "val_data = data.Subset(val_data, indices=indices[train_split: train_split+val_split])\n",
    "test_data = data.Subset(test_data, indices=indices[train_split+val_split:])\n",
    "print(\"Train/val/test sizes: {}/{}/{}\".format(len(train_data), len(val_data), len(test_data)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTD1GGkR8Ss4"
   },
   "source": [
    "Finally, we use `torch`'s `DataLoader` class to create a dataloader.  The dataloader manages fetching samples from the datasets (it can even fetch them in parallel using `num_workers`) and assembles batches of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LhoykOhv8LdN"
   },
   "source": [
    "num_workers = 2\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcjHBMnAawgS"
   },
   "source": [
    "## Visualize Data\n",
    "\n",
    "In the cell below, we will visualize a batch of the dataset.  The cell visualizes the input to the neural network (the RGB image) along with the associated label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "qphYetp0ayIZ",
    "outputId": "0b97ca04-a80f-4542-d467-2acadb470724"
   },
   "source": [
    "n = 4\n",
    "inputs, classes = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(n, n, figsize=(8, 8))\n",
    "\n",
    "for i in range(n):\n",
    "  for j in range(n):\n",
    "    image = inputs[i * n + j].numpy().transpose((1, 2, 0))\n",
    "    image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
    "\n",
    "    title = class_names[classes[i * n + j]]\n",
    "    axes[i, j].imshow(image)\n",
    "    axes[i, j].set_title(title)\n",
    "    axes[i, j].axis('off')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O-ZKD5KqdMm"
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Next, let's explore our dataset a little bit more.  In particular, how many images of each class are included?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "zhHebM9-qiAv",
    "outputId": "271371b1-31d9-4a4c-d930-0ea482bcd484"
   },
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "hist = sns.histplot(dataset.targets)\n",
    "\n",
    "hist.set_xticks(range(len(dataset.classes)))\n",
    "hist.set_xticklabels(dataset.classes, rotation=90)\n",
    "hist.set_title('Histogram of Dataset Classes in EuroSAT Dataset')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ3ef1a34gix"
   },
   "source": [
    "# Model Development\n",
    "\n",
    "## Instantiate Model\n",
    "\n",
    "First, let's instatiate the model.  To start, we will use a standard neural network architecture, called ResNet50. Based on [the work by Helber et al.](https://arxiv.org/pdf/1709.00029.pdf), ResNet-50 has been shown to work well for LULC classification on the EuroSAT\n",
    "\n",
    "### ResNet-50\n",
    "<b>Recall</b>: Deep neural networks are difficult to train due to the problem of vanishing or exploding gradients (repeated multiplication making the gradient infinitively small). ResNet solves this by using shortcut connections that connect activation from an earlier layer to a further layer by skipping one or more layers as shown below. This allows for gradients to propagate to the deeper layers before they can be reduced to small or zero values.\n",
    "<br><br>\n",
    "\n",
    "<center> <img src=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/images/resnet50.png\" width=\"600\"/><br>\n",
    "Image source: <a href=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/\">https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/  </a>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Note that when we load the model, we set the `weights=models.ResNet50_Weights.DEFAULT` to indicate that the loaded model should be already pre-trained on the Imagenet dataset. We also modify the final layer so that the output matches the number of classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPXbNl_TLHQQ",
    "outputId": "20595f28-b040-4fc1-e612-f31057ba0fa1"
   },
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGf__UVfNNOO"
   },
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We can now proceed to model training and evaluation.\n",
    "\n",
    "This section has three major parts:\n",
    "\n",
    "1. Specify the criterion, optimizer, and hyperparameters (e.g. n_epochs, learning rate, etc.).\n",
    "2. Train the model on the training set by updating its weights to minimize the loss function.\n",
    "3. Evaluate the model on the test set to observe performance on new, unseen data.\n",
    "4. Repeat steps 2 and 3 `n_epochs` times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3waPRdOai7AB"
   },
   "source": [
    "### Cross Entropy Loss\n",
    "We define our loss as the cross-entropy loss, which measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. ([Source](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html))\n",
    "\n",
    "For two classes, it is computed as:\n",
    "\n",
    "$\u2212ylog(p)-(1\u2212y)log(1\u2212p)$\n",
    "\n",
    "For multiclass classification with $M$ classes, it is defined as:\n",
    "\n",
    "$\u2212\\sum_{c=1}^{M}y_{o,c}log(p_{o,c})$\n",
    "\n",
    "where\n",
    "\n",
    "- $M$ - number of classes (dog, cat, fish)\n",
    "- $log$ - the natural log\n",
    "- $y_{o,c}$ - binary indicator (0 or 1) if class label $c$ is the classification for observation $o$\n",
    "- $p_{o,c}$- predicted probability observation $o$ is of class $c$\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Remember that the goal of stochastic gradient descent (SGD) is to minimize the loss function. To do this, it computes the slope (gradient) of the loss function at the current point and moves in the opposite direction of the slope towards the steepest descent.\n",
    "<center> <img src=\"https://miro.medium.com/max/1400/1*P7z2BKhd0R-9uyn9ThDasA.png\" width=\"350\"/><br>Image source:\n",
    "<a href=\"https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a</a>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fap-QROui8Pr"
   },
   "source": [
    "# Specify number of epochs and learning rate\n",
    "n_epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "# Specify criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next. let's create our training function."
   ],
   "metadata": {
    "id": "par-N39lvb6Y"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eE4entSvNc2q"
   },
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "  model.train()\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_total_correct = 0.0\n",
    "\n",
    "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Zero the parameter gradients\n",
    "    # Clear off previous weights in order\n",
    "    # to obtain updated weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Compute the gradients wrt the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights based on the\n",
    "    # internally stored gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate statistics\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Calculate running loss and accuracy\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_total_correct += torch.sum(preds == labels)\n",
    "\n",
    "  # Calculate epoch loss and accuracy\n",
    "  epoch_loss = running_loss / len(dataloader.dataset)\n",
    "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
    "  print(f\"Train Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
    "\n",
    "  return epoch_loss, epoch_accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D2XrpGxtJ9w"
   },
   "source": [
    "Next, let's define the model evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uvjKlBsNP_pF"
   },
   "source": [
    "def evaluate(model, dataloader, criterion, phase=\"val\"):\n",
    "  model.eval()\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_total_correct = 0.0\n",
    "\n",
    "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_total_correct += torch.sum(preds == labels)\n",
    "\n",
    "  # Calculate epoch loss and accuracy\n",
    "  epoch_loss = running_loss / len(dataloader.dataset)\n",
    "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
    "  print(f\"{phase.title()} Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
    "\n",
    "  return epoch_loss, epoch_accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting it all together, we define the `fit` function for training and evaluating the model on the training set and validation set, respectively."
   ],
   "metadata": {
    "id": "z9Zj-K8uox_h"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KNgDSfQDMSFI"
   },
   "source": [
    "def fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer):\n",
    "  # Keep track of the best loss and\n",
    "  # best model weights with the lowest loss\n",
    "  best_loss = np.inf\n",
    "  best_model = None\n",
    "\n",
    "  # Train and test over n_epochs\n",
    "  for epoch in range(n_epochs):\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, _ = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "      best_loss = val_loss\n",
    "      best_model = model\n",
    "\n",
    "  return best_model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzTl0FX3tN8m"
   },
   "source": [
    "We can now commence model training and evaluation in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f37f2136ddaf4caca182b4d2defb365c",
      "873fbc7dce4b4747bbbb31e7efcedb75",
      "cd18a4b817d748b589679edbf8ada7a4",
      "86337092891341c9b033b3dca9afdac2",
      "78f3480489cf46739010ec5849536d10",
      "22c256445afb4c979a1bc0a6d7bc05b7",
      "bd9f80108d24480fb179e254b3f301e2",
      "bac59af8af21406c828564070c0b2e2f",
      "e841ff9e57e44b698958da0d387cc76d",
      "070d2fcf6932467897d0490f4426883c",
      "0a5f061d094c4281b7c74e2a824db951",
      "59f774161ade4118b28c8638fe649d27",
      "9862771cff9247ae83a1a02b7723d307",
      "37c9eaafd6194157bd9ea322b352de90",
      "44ab9f05a02546a1991af1bffc534b77",
      "ee86f0f750ea4ce68323684c9d5faad3",
      "fab5ca9869614c3b8e8e2a0f284ea1f1",
      "c1fadb1f91834da98dbcc90ad855066e",
      "1934fef956fb459698bcdbfe2c860bc7",
      "6aa6b99b2a454190a683622a63a1df65",
      "c7ed71f1add045bea5bf300df8fbbb5c",
      "dfcdbaf0613146a0ba33ac9798594c01",
      "87a6eaa15cf14facb2b8a33ed3726a08",
      "d2370cd21c4a475f807244b9fe70f79f",
      "ffd17d56c57246889fa23378a19ee223",
      "de849d015e9e4ee39b0347e66fd52a6e",
      "b492ff0a14644573a8e86e752e955f7c",
      "ca6787b7b98340a28b5547a05c7cb030",
      "e1cbb458b7cb417c82e0cc0ab59dbaa0",
      "aab6c94342e249ed89c033e74c70e863",
      "74f28f7bcdb841ed876ff6af06240204",
      "f19beffe793747b78ba59e9b7b9dd466",
      "085925a92dde400cb4f8df76a7aaf293",
      "4c8aeeb46e5a440bbb6ca91fe0e877bf",
      "0166b04ec8174033be7407872ed5f92a",
      "f7009b2e02b945bc8a8a4d3a486aff18",
      "657d054ddec643f2a64f1627f78db0ab",
      "f6b5153017824d5fa15a3d0676a3c7f5",
      "80286692186f4a0aa7116b165b52ffca",
      "f063d46f1d56422398f000cfb39ca247",
      "e36ae26d849843279a29ebcb31a194b8",
      "f5d3170372f3416491a327a7d138e878",
      "b7c25af86c7d4f3cac2eb049b5e82130",
      "af518a1a5a8249b29ab64d3cb8be6ebc",
      "4bf3b63527ee4be09a540c440c1f0351",
      "f81564215735431788a3b21bdc5587ce",
      "e7d4be388a384f58b5542ec82bba5b98",
      "6c7a1d692b7949b090577fac31320a1b",
      "9753eb9dcf3b4fb9994f4cf9351107a0",
      "710ed4699c91499ba5b22e7eb54d83fa",
      "1c36918480fe4f03866a22f7369e99e7",
      "1d6d800577d540af87ce79dc8e3939cf",
      "0756a5a8010b461882f2bc1ea1e52b06",
      "0ef5a2ddf15e45c28a4347b7da9c7c01",
      "2f7f9614ef28461db0bd50f2a86a07fd",
      "0d7673acb69d423f98a6d10b85596b36",
      "5a12c37d692b49a88e69ef0e5fde033e",
      "b512e0e6cd1c49ce9901652f9ffe9f0e",
      "234deac0b0f04eab86586df53a7cf8d0",
      "9d0467e891214a6094da8f4e01972dd4",
      "91994276cdd4424daea85e90f3bcd471",
      "3e4c71de27a44780a7ef41ab47697e23",
      "f544d29b1f774e7297a6469bd20c4a00",
      "92c03d6c0e4e4756aa7d8a64a46b5dae",
      "27e2d4782ea744e49b66c54c14613187",
      "978b0a42989a41ba9987545589b32ce0",
      "19d0ac20004345c192d2d3a68a336d38",
      "e37437e308714d37bbb7309376bec41f",
      "5b1d6c7eefad462685a58f9cbfb92b2e",
      "0ccccb17c6544d6dac5a6cd8e2488868",
      "54dcc43f1fe04530869be4d4ad64636f",
      "5830cbfbe75f49bca5033d85819a09da",
      "2d206b7b210349e289f466f2a2531a13",
      "7dd57fa918ac40ca91ccb84aa9c15286",
      "9c5410e770864ab1aa9c1e5d35211522",
      "b6f27a0d911146788d1455cf11ca828f",
      "cb7e14b1d588458197cfd716b4b5c92d",
      "7a896d3b06b443c5b4004e2d5142ddf9",
      "da6d139f355c42cfa85e26d224a95764",
      "8c0b3df9c67e420d8319ab0d287af42d",
      "26f047be142141118c0a3c39319d1208",
      "71e5b8b571254832a813152932d9fa13",
      "20f8bc0102c74d4d9fcb43d6807108da",
      "70fa58bb02b748dcbbbb500e17699f9c",
      "b536ec58493346c2b38fce2600134b2d",
      "5b64e8bd1a234ce688b6c4628931107f",
      "88cdd69e6fa443bd9750b8aa2f1643c0",
      "ec0d2a05d89048a1a09b9d824195d4db",
      "d609c40cc47c48cbac248f73cb1d1641",
      "9b4690b1040d4c6bb87a7419d61c2c3a",
      "5e002469238a458e821880911fcd4693",
      "1e31d8bd7e2c4fdb98d313db2fbbfb83",
      "7a13b4337e4b4807b31d4f5b1112d1d7",
      "ef397898a62740eca4ed3220893105f1",
      "383a5fa9f5cd4641a28c4c03e76c6ff8",
      "d38592545b85403b96cc98cafdaa2c0b",
      "89dc6321db284ede91203c5c033143cf",
      "7e24fe383059446ea167c935e4534ec6",
      "1e67165e5f86443ea04eb86dbda58826",
      "54a58d38f7354caf92a5dc2859e3fe01",
      "bacf55aabe2e437ebec95b2ba06b0df5",
      "825017e1ec6c4aa3a759d0d1cd888992",
      "b620487bc8104085a6676026f57c22e9",
      "b82667ee501848f8918d37961c324c09",
      "4d7a8347dce74240814d2c367297a10a",
      "85fdae69947f46afbf4f7124bbae5668",
      "3b103202cd7a4ac689cf58cee04709da",
      "93da991d7df841b2864f940f7e134336",
      "e35e2e4b16f846e29d849b3f7ad591cc",
      "707b4d3b6df84ffdb9f06a16b915737e",
      "3ba6a57846c247dc8a1cfed0dac87350",
      "286ddb55a9584b5ab90f10dd7ea23ebf",
      "04f76cb250df41b6a5658bb1f1e64235",
      "5a88ed2e15fa445c90a5b63db0cdf469",
      "9501c907242e446d97c25e68b9e099f0",
      "16610eff3a294bfa8869354023fcd056",
      "8cf9747043354ac5ae3169431f290af3",
      "877f8381fc8c46c68eee10c1262e958b",
      "8f1008708bfc4cf79526f11b9a6d3db4",
      "7d681770b29e42b8aa4a6b8d9d7701ea",
      "5b08b7606f9943cc86452e19926fdd1b",
      "1d10b536eb9f4c388982fe96e2ef3650",
      "0e7758d337b74725946c48f10590c0ae",
      "3caa7493bd5d4d9c84b5f0e00436e42d",
      "e31a68cf30ea416f87322a96170050cf",
      "c43c577c73b54426ba5d56098a22406e",
      "a8f32d8a4bd747f6a876fa8d51d13f75",
      "69f32f52098f447e884652b4abc43b43",
      "a43ceb3d0eab4e34b93e38c5b8fd4e90",
      "4916d58778af4e9f8b40cd99b8cc7d0f",
      "1337d96c531a47cb8d5ea2768a08da1b",
      "cfba0d23edab4c049b2d75a8b6215e02",
      "04055ff9d11c4ed391ff3b91f01a3575",
      "f1d24dee1d4e4b608510767b771da886",
      "1acbd3285d1b4fd9a1604df34b67bb05",
      "01b07a9fb6ca4de599dd31b5739943bb",
      "1991d4acdf9746b6ac79dd244b351040",
      "c8aa50c9f0e14ba582a4a2418d2a9b5d",
      "4fc3313f8db14fc4a0c6a3d749b9e8c6",
      "52ee3979690947fc8716131a6a0833e2",
      "d4cb9f22ed1a4037baa56919f9f1055b",
      "050733f835a041099f0f07d907191275",
      "db4ce1623457462c9e5cb71d874234f5",
      "8cbe15c9d0cd4a36a174288256239206",
      "0dafe5bc64924b3f9f2ac9ef21ae1e3b",
      "c92dfb631fc243b9a48b60d5e36e8a2b",
      "24afed46320943bea7b38d07b798d3c3",
      "74908fb4f3554c20ac087fde654dc825",
      "8e4494fd57ab469ca3eec29c8daded31",
      "58c5d2fc531f4728b66b2699cb76ea6f",
      "358d23fb54ac49e0b33eaf296c893170",
      "7ad0847e4bf44aa4a75f12f11d6b201f",
      "fbc233ee986a4fdfaeb7c0ac4c90f277",
      "99f6f087b5d2402ea92e38b1e3a56f28",
      "4545316b99e648ee946a345d83b96785",
      "5c0279ef2113474eb4854343ad491818",
      "f16441c9c01f46fcba8d1cdb9d3d10c4",
      "282319a76efb4c1f905f9dfba9d2eb0b",
      "671c5779f0fd496c8c64c2637336d578",
      "0f557797299f43ee8f7ba1d83f447d93",
      "e27a52ec26e64eee8c256ac912235d37",
      "f0cc409f4a384f1c84c1dcc08d5b5136",
      "64b8bd8d4c8e4c83b7c00852c1423693",
      "6002226fe7a74d86a009a6a242a0696c",
      "c594c596bf614e79b29fc48f91bb6945",
      "e267e52994b74fed9be4036c68588de2",
      "87462e20635045b4a5b618454fbc62b7",
      "d1942368b72d44cd85e873d5e010bbc9",
      "c9e099bed0b1488782bd5cf0f5a1dbb5",
      "418a6e1806c5438197c293e3750aee51",
      "3fba1142706444b3a2056110bad02989",
      "847a53a4e342475dbf9b556db6628e6d",
      "8855816349854a22a5d2d3860d22e0da",
      "9ddeb498c02e4067ab2d0396f1c79592",
      "6e7bef79669241338b502de188719d0c",
      "42a33d22594f4acb8b3444df510a1a6e",
      "a67542ecd5f944f0a951128f6959bb74",
      "cb2f6ac0e4ce4f1a8da707cdef78ad7b",
      "3b99563828614c25b5486e596a307623",
      "cf558b70ddb740e0a2d711918252818b",
      "d5002c07e07447a4bb034ded90cd2736",
      "52e61f48524f40d6b12fe6990b44a0e3",
      "a9b03efcb8e14c2b8cca96d00388d3e7",
      "e7ccf77887784459b72d63b06defc61e",
      "e9cf14eeb77c4a1ab8301e36cce09e96",
      "088ae920f4f94cc68f66264e35202430",
      "117a0e6b402c4736b6e2ed4113e4586d",
      "d9a9c779575a42e7914ad054e2b46148",
      "cff05f1378574b2495d2afb17c32d5a1",
      "362c22b042c24e09939e9cc979612ec3",
      "2819e09b566b47928a34f4f9142fcdbb",
      "3fcc838291c943ff84e498b35f67b538",
      "28dab403f2b142d0b504af8d113a3b43",
      "6baa55eee1de4038a8b000dbe034ab3b",
      "1f602ead3bce4c9f9fabc517d035b517",
      "9d2d6e1ce32e4a4a855ff96b4eb726bd",
      "5b773acd83894961bf2d481e52ceda5b",
      "8fa2ed819d8642308ff4cb2f515fecc1",
      "70b5d4a062d1444cb3dc784c21bc0835",
      "cf0406cabe5b49dfb8b826bb0f91fb93",
      "3d885f8e39e34ea28ceaba92b8cbae31",
      "b176423e9f794c52a731ed2b474fb1b0",
      "368cfd8865fa4690bf438e7560e23cf6",
      "ae9a5c62058941e2a5e1eb91bb1575b6",
      "9dc29324fb184785b6303735e7d5c907",
      "a2b2868e17574443af8ae928834dbc34",
      "8f3390dac4fb4605b5dc1a99c689978b",
      "6cc2790322a6406589ff586a561884bb",
      "038dde6e19c7441e9107f9ada336ec7b",
      "f6ab6c5b6ea4488eb88a2a4a0a5c6811",
      "99040edfb1724f899af42156438678f0",
      "2707e76cb87c4346988323afcad841a1",
      "3db5a8738ab741b1a59381045e20ca02",
      "a51bad2ca04d4870a934c5e2351bd409",
      "38af2391a6ec4300988d3b78d06b963d",
      "abcd0c20e61d4ed1b18166db1b8ca001",
      "ef7caf44a62f4c368a87d175c357aa8f",
      "10f308e87ab04f7790fd83f4f6131f1d",
      "5578ce4eaf1e47208eded89c7c5132c3",
      "85afb89cde7d4d2381661d11e8e010eb"
     ]
    },
    "id": "Uy0FvQkumiN6",
    "outputId": "a0668685-29fc-48b5-929d-bfe11294d2f1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example, we only trained the model for 10 epochs. In practice, you'll want to train the model for much longer to achieve the best results."
   ],
   "metadata": {
    "id": "J3HPljIKqYyK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Performance on the Test Set\n",
    "Using the best model from the previous steps, we can evaluate the model performance on the test set."
   ],
   "metadata": {
    "id": "o-4SbmYgu-ZS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss, _ = evaluate(best_model, test_loader, criterion, phase=\"test\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "065e6f308f82411b95ae9a298624dacc",
      "82092ddd656549369f796a070a68e26f",
      "b764c0a2d5ad4e7fb0d1f40694211dbb",
      "2f87295a74ef419da525a51397752d63",
      "1e6888d17cc24479953a987aba7bcf0d",
      "ced85f820aff46a5ae41b4fd02e0f830",
      "3493b71d389d4ccebbc97d28da012b54",
      "9db1ef414a574a68b75e737a1178c881",
      "354011cffa2343e58dbf4f2018f3604f",
      "2cd4e225a0db44b0adbbdb1cf5f0de6e",
      "b9870ea2b5284115a0c81047b8116a8e"
     ]
    },
    "id": "Cdf5Qn8Juq4K",
    "outputId": "ab58c468-ad8f-4de1-fdd4-89413e490e81"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14oPIiaqb-hI"
   },
   "source": [
    "## Save Model\n",
    "\n",
    "Let's define a function for saving the model to our local Google drive as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_dir = \"./drive/My Drive/Colab Notebooks/models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "  os.makedirs(model_dir)\n",
    "\n",
    "model_file = os.path.join(model_dir, 'best_model.pth')\n",
    "model_file"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "riBvu9R8rV7h",
    "outputId": "20719059-539b-442f-8a6a-68b4b698e169"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "txqSMimEO5B1"
   },
   "source": [
    "def save_model(best_model, model_file):\n",
    "  torch.save(best_model.state_dict(), model_file)\n",
    "  print('Model successfully saved to {}.'.format(model_file))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "save_model(best_model, model_file)"
   ],
   "metadata": {
    "id": "em2xesOoRNr2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f4118fa8-4743-42aa-e238-971b4de63cfb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeqCoai8hcYr"
   },
   "source": [
    "## Load Model\n",
    "Here we show you how to load the saved model from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oRRJ0vlCheDD",
    "collapsed": true
   },
   "source": [
    "def load_model(model_file):\n",
    "  # Uncomment this to download the model file\n",
    "  #if not os.path.isfile(model_file):\n",
    "  #  model_file = 'best_model.pth'\n",
    "  #  !gdown \"13AFOESwxKmexCoOeAbPSX_wr-hGOb9YY\"\n",
    "\n",
    "  model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "  model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
    "  model.load_state_dict(torch.load(model_file))\n",
    "  model.eval()\n",
    "\n",
    "  print('Model file {} successfully loaded.'.format(model_file))\n",
    "  return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = load_model(model_file)"
   ],
   "metadata": {
    "id": "9CnI7XOQSiP0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7c269a9-7e37-40dd-f1fd-b72ac65c02f2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6c20o6xcnZ6"
   },
   "source": [
    "<a name=\"results\"></a>\n",
    "# Results\n",
    "\n",
    "Let's visualize an example of the neural network making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "77fAZ7xEglBX",
    "outputId": "4aa4e5aa-738d-48fe-f3a7-e8370486683e"
   },
   "source": [
    "# Retrieve sample image\n",
    "index = 15\n",
    "image, label = test_data[index]\n",
    "\n",
    "# Predict on sample\n",
    "model = model.to(\"cpu\")\n",
    "output = model(image.unsqueeze(0))\n",
    "_, pred = torch.max(output, 1)\n",
    "\n",
    "# Get corresponding class label\n",
    "label = class_names[label]\n",
    "pred = class_names[pred[0]]\n",
    "\n",
    "# Visualize sample and prediction\n",
    "image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(image)\n",
    "ax.set_title(\"Predicted class: {}\\nActual Class: {}\".format(pred, label));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvUL5a5Gp5fC"
   },
   "source": [
    "Here, we show how to run the model on a PIL image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "YuMyKEFjdvvn",
    "outputId": "5bc852c6-9d19-441a-a44a-7de0d06db56f"
   },
   "source": [
    "from PIL import Image\n",
    "image_path = './EuroSAT/2750/Forest/Forest_2.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Transform image\n",
    "input = test_transform(image)\n",
    "\n",
    "# Predict on sample\n",
    "output = model(input.unsqueeze(0))\n",
    "\n",
    "# Get corresponding class label\n",
    "_, pred = torch.max(output, 1)\n",
    "pred = class_names[pred[0]]\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax.imshow(image)\n",
    "ax.set_title(\"Predicted class: {}\".format(pred));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"exercises\"></a>\n",
    "# Exercise 1: Experiment with a different fine-tuning strategy.\n",
    "\n",
    "So far, we've intialized the CNN with weights from a model train on the ImageNet data and retrained the model on the EuroSAT dataset by updating **all weights**. Another approach to fine-tuning involves using the pretrained convolutional layers as a fixed feature extractor and freezing those weights, only updating the weights of the final fully-connected layers for classification."
   ],
   "metadata": {
    "id": "RGWzTFVsVUy5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\u2b50 **YOUR TURN!:** Freeze all but the final layers of a ResNet50 model. How does this finetuning strategy compare against the previous strategy?"
   ],
   "metadata": {
    "id": "CZKCNy0LVlLh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Add final (unfrozen) layer for classification\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Commence training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "NHrXwXxH5zfw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "cZzXC7IHCazM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add final (unfrozen) layer for classification\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Commence training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "KrIB6R9BQ_g7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "81e8707fc663463ba7120fc7b3ddad18",
      "a63ea3617c844707bf12c11b6b4c14c3",
      "e319db493701473f9cd8d7a3dc8785c0",
      "feecce1a707e483a9a44b09eb49a28ab",
      "ad7634f616404b14a4516b790c45f010",
      "0c9f68148be8486c84da8a03b0666a38",
      "83c65bd33c6c4b8297ecf92924bd8e78",
      "e170d955c74e4f0297bff73cd97b2b78",
      "2226aa1f907d4c0293f0bf7bed9e4ccc",
      "3d2e74cb471446349bfe46eaa4ea65e9",
      "2332ccbf6f96469fb418d12cf5781d21",
      "6e63e5fd598f4387bcab26415c90a007",
      "db0a868627d94ff88e1d9a87b2536f04",
      "1d7da641556e4ad59ce68dcd23729a1c",
      "c3e53368f8584ba8ad20d35536a131bc",
      "42c0e5b1776343ab942b3419a09e1f7b",
      "1dcf04bde17f463190cf494601c0b162",
      "b663e76204f2436d93d42e1ecd8e044b",
      "a9d48409d62e486c8da848574325b78a",
      "de3599ae02ff46d0a393b0c6e55b4314",
      "ef6f33bf0b68441d982061f4d3bcffb9",
      "b7087f5d19ed4b19a2333bfeb37037fa",
      "b1d8052801174b34b287e84af744791b",
      "e50cc79071944b888d610d2ba70e1ea3",
      "f9778f83c5b342beaa542964cbef3c54",
      "4c5eb04edd774517915eb5108ce72d59",
      "d6762efe3d1446bebebe15ef249985ce",
      "61ed59b2c8df4bad8d3e1d7a3b6e344e",
      "0496117728d842ee99587da661f1d70a",
      "7f3f6c1f54fd473498faf855c0599119",
      "7b9813c102144cb39e94aafab8f494e4",
      "20b8b57cea4c40f48d14e1ee54b37a9e",
      "e0fe315743224548a6ec7a36b26b4ac0",
      "fa334990edc94210afb535e691d76f54",
      "7239979a36b747f0a255229f85682a9c",
      "c1cce23693cb4faab5d9815a678c2416",
      "7749f819d2a44e52bf3df488fbf29223",
      "2e8a2d6ae9874789b5af2a0f16b9a11d",
      "09287c3a47e34ff8be93d0c23c9d90c8",
      "538dbd465e0f402fbf0443b114c551d5",
      "2c9c217cc88e4f4096dba339f5d34a5e",
      "6762cc54bef4405fb16cf88200ef5b19",
      "c75f0880f47b46a4ae0324637cd14ec4",
      "1a3f5084cedc4b928256ed8bb37672af",
      "9eec595779bc4d4f82bf2d19fc541a75",
      "bdba2078d7664aa18cc650e0a651247f",
      "38d2f30fc06d4ccaa62f577306a107df",
      "b2da8660a12b4c6197b4a43d0dea9d12",
      "716cb6dc29e549cea841f839259a1544",
      "1347f376ba1b40458b953a57dd469eed",
      "6676fde05b8442e1bc1799c47d9ca088",
      "e3256f2dfbea42f19add25f33cff228e",
      "16cf89d0d5544bb5bda16cc0db4dbc7e",
      "8605c665381f4cfbb6c2d00ad087c5e9",
      "71206044cced4eefbfc583a430e1ef8e",
      "8c2f85204cac4f7cb5f90185b040f930",
      "7417360d6e054c57b59645aa5eb1c54e",
      "ba28e515e7f24d78990742ddccd6a837",
      "f4056093bf904a83960821b33380d43e",
      "6935875c2f9940bfa82ccd3cfad55278",
      "4fd77c0f290c4db0a16bcdb87198e0b3",
      "4c305dc7ddcd40b2ad2f2b2ceed91dda",
      "3814856e70244ab89bbe87753bd5319a",
      "e1378f5b27ad4f41b485d089315857e7",
      "d0aaa216bb964af996b49353d8591a81",
      "d02b2a5b6c2d4f0dba5cbaa29319ec34",
      "fd19f6fe87b34fe0b8e72509125f6cfc",
      "7adaee1771124ab892c8f30c1a9ca5c7",
      "ce878e757de04e41a58597bbb0a7d026",
      "278c443675504ea28413d789c65f0a55",
      "6d2a1c4b111144bc9d223e34f80dfdd7",
      "1a79459323744a93ba0de5741d588dde",
      "7ddae03de52f468f9c5875081cbe1657",
      "211eba684b85452b9be23353b76737ad",
      "5e85040c067248b3b4c6a7265a5267fa",
      "12f3db7aec6548e58cf916777c67bb7f",
      "119b605e4915414fa061f87bedca5b07",
      "8a1191238f2c490cbe5fefa08e149ac7",
      "51439d7579144a7793a338240d78656d",
      "44d506ce6936415bbbdebe611616c431",
      "659d3e274ae542c089b163a3b5e5f5a5",
      "bf869cba5da840b5a4a36b1b2e65547b",
      "993d113fbd6c44aa8e8ba3c04572639f",
      "6f1fb6c2485345b39c0f21c36e239b5d",
      "46e9a850cb264c918ac9bc51d58813a7",
      "f03e538591bd49a8a90144922f7bf9ec",
      "52ea97c637c64bdbb7c18feb9a465910",
      "b05504d89de94f8ab70901fb537a2de6",
      "6f1ba7a1ba3c4d14a19fd2445a15fd47",
      "9771dfd4404849deabc22a7d70f7dd9e",
      "9bb0b077f5524076be3982913faf3ffb",
      "a01ee71646804dd7b89f3b5fa1957c35",
      "d0fe4757cdb84cd2880c10bec1a11eed",
      "14168e15a900436d8e4e98db701ea53f",
      "4c882919612f48c2886acb2db5e00dff",
      "009f8c41d65f4001a4df295bf08a5257",
      "1b83dcd505e24dfca7d0e4a1ecad3fab",
      "b70624d4f608485c9f91e8f77a0f5766",
      "a5c66b5b29b54743a9992a15fdfa751e",
      "112e635ce64d4e54be22c3208949066a",
      "852ddd94e56046f1a6e76752c33781fa",
      "5d1c4f330b0248e9b869f338f2fd2895",
      "638f8f2a6d9446159db4e9d9ff5f8272",
      "08b055e020994f988928c692f7f47133",
      "bb306f488cd84a09bc43cf2bc6ae8871",
      "ae8cefe48d054d59a110c4d5ccd5e116",
      "1076371c8fe04b61959a9d3c15f8dd27",
      "d6a73c0d3e7b4841a7dcdf0c736e88c8",
      "4e3a136b90bc49c3b42a61f57d7a97e6",
      "1703873f869044639b2aa9deda66302c",
      "db4af49cee2f48a49f0a14f3bd5c685c",
      "17d6b0a6cc5c4629a7301859acf057fb",
      "f2a5104d7e444325b222d79da6928fef",
      "62b518cb716b47eabafdec3be235a614",
      "79c9cf32f9ed4e85b16c5918bd9df529",
      "09768edd58c545ed885e3be08b01917b",
      "c8330529b1764d44b1bfb4471894b3b5",
      "dcc63e75350b4a64b347ed1766dc7085",
      "71fe9f4a854e45a5b21190f72ec8c2f3",
      "a82cd408eb3b4935b7c5c5396303af09",
      "aa122d3d73b748c0abf579b5e9f6b34f",
      "4e43b667076f4269b3d2badc09a68212",
      "2068748e703f40968c87e52309240c62",
      "865b6644f4594af3a0f1c51d674d522e",
      "5f6f7e78b5eb4f9798fb166e3f26a6b8",
      "3db027e36d334e189cd9ccebe0feca53",
      "8af415f86023473482fd95299dd8b782",
      "c7ea4fb61dc14a12a9f3b04981ba260b",
      "46d587a944254c6b9e6ff36e00132d73",
      "346e0a4ca9e74cb094b9c71cc54f4044",
      "999cc1a5bcb64d37a91943ddcfb7ec9a",
      "390f1fd5048d4383bf3d4c08f5eaadee",
      "2fef00b4f85d44148cadfd420bb599cd",
      "50f795bdfa5c40928a47d0ca41e88fbf",
      "4e653ae8d6374ec38aaa697af8cf0c33",
      "83a5e0e2cea342579290f0ac8c02704e",
      "e5e2b7111e614d34a5c12d10831909aa",
      "37ffa2f3a1134394b44e3a0474e6dda4",
      "03fb7b1fec1e4633bc1525d5b19ecfd2",
      "6cbbbb895b2f46beb370b58d6f20f9b3",
      "4e94ee0281ad44748d032491b4ff3a1c",
      "055f5514033b4a01aeb42e46435e6f1d",
      "0979f1126ddd4979a91cbc60b2948f5e",
      "f814c69542eb4e888c64b999d4cccc95",
      "affb65a2f57a4b1e8293ccda61ed6f60",
      "4bda1546d6fe4651ba54f6abc954370f",
      "3dcafa2ab7124913bd280c541d99039c",
      "751b95ebd0384e799d6e94a31731b3eb",
      "69092d0b53b74ad98bbb9dd916711cd2",
      "e9027997e0c2497caa44232cfd0afe4c",
      "f40215e846b4430cab12b6c54460dafa",
      "bf84b5badb354264bb143a04a1b4251b",
      "31e4830abe0c48f687054de5acb4ef48",
      "9aef5c18f9d6471eb8fbb83372dcb30d",
      "81f072c5508a4db3bfb97d0de37bdbe1",
      "8728a1eab8c845ca9e39b94b98351ba0",
      "1891f3ffa9cb408397884bf5fbfb699e",
      "a009be80495c4bc18bab25f861c17062",
      "05965c9fdcbf41f2ac74ce59d9de7934",
      "9ccdf6d33dd744cf92dbffdcaa4a802f",
      "ecfc95e12f3642f1906f20887c1f0b49",
      "761a16e9b46e4144b13b21bfec71109d",
      "5072c25942b24a6c947cd3b5fd09862e",
      "9e3d3060a5994e7baea377a5e217708d",
      "469475ee33cd4f7fb4b460646a146b30",
      "175b36cc699a49608407d46a3fe86bc1",
      "e019faeba55243c68651c709c6e2ae9d",
      "4a5c03c8cc224df7b89f116b93582614",
      "cfbc286158c9486cbf20c7619c25ae7a",
      "757ab03926a24170bf7c58c83a031eb0",
      "358811f616ed46bf9bf8054a6807fad9",
      "54308ff3529b42278164b0af5a144372",
      "425904ffbb284915bffe81bf45db13ee",
      "3c91e7ceb3ff4516be43bd53221f6ce8",
      "32c2bca7d1cc48fc805bc5d42507b52d",
      "3b051bf94a7a4ce9a0e1f0e87b0cf08a",
      "a1f9d92c256046a6acce52c8178b3b98",
      "eb1dea68463841de8943a76bf3fb9a52",
      "6188db22fea84cf9b4ae2cd865c8a0e5",
      "e18edb3f00754246a65a1d3407790d56",
      "cc2a5f7fe0df4f378a641401ed22f986",
      "eb20b64e16754dddb1135b9daaa02eab",
      "ec6d0bd5c0294dc0ac14ec53e0035f90",
      "7a481335e13648d486af711f4ba57310",
      "68646e87a6be42479abd49c2eeeed5aa",
      "cb77de25062d4061bc2a10366d2db971",
      "b8558eaa632247918947f83afd5daa85",
      "b4d81c1bbcbf4be2999e8798e5dfc286",
      "0db989107a734a84966ecb44c0a28da5",
      "4e3cafbad34448ab8093e7b52eacd320",
      "d0d7118ccc7c4e699588465731eae49d",
      "99b52c886b2a4bf9873e85de27e2fff3",
      "d28d7c06ea244f85b2e8c884e865f54f",
      "7b15c6802a6a4d8585eddc50ab8227b3",
      "ab3cfe3838484ad583deb182c36f3e65",
      "ba5d49248bc942458e51215914a65c16",
      "79705605de9042f7939e0ef18473f99f",
      "22735c76411c4bd9bbf143291adeec3a",
      "d45ca58900444885a01bd5ae004c537e",
      "5307bb2bf3334258b370333f33c491b1",
      "97fbe6e77db84f308c5141b0835e1e5f",
      "883dbaf7a55c44858836362964275273",
      "a0f0c4c565174fd897360fcbd87fb31f",
      "235a9fb08f35492a902668141133c2cd",
      "ba0ddeb451924f65804f9037b4b06cb2",
      "06e3ab05356d4f57aefd01f2b5afea7c",
      "bc429b4896494baa98069f5cbfb4fe49",
      "c5b5e1b93fc44e8f9fc6bdd43b1485ac",
      "f68017881904455cbb854a5726f7f7ec",
      "04a4edb1cc5e4efdb7ea8954548fe19f",
      "31f0b170c0d640a28a75a5c42d8d58ef",
      "b10ddd8cc407465b864d0090577c381f",
      "43d1af416b8043e09766efa66e7de885",
      "05427ddb77d14a1da1b6a295028445df",
      "f6b44486468a4d4aab9f1958e944e3a2",
      "72a9ee79ddef40598c55c43db3800917",
      "141a76c7ad00418c86b2f5078f948fa6",
      "e7127ec927ac40289341adedb1af5696",
      "f8f237598f0c476a9de3f2fe3f7ac684",
      "3e08dbee08814cd19fea2926566b591c"
     ]
    },
    "outputId": "6eee4bae-61ac-4c3d-e9fc-f60f29019e44"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Experiment with different Imagenet-pretrained CNN models."
   ],
   "metadata": {
    "id": "O83JctiPVJ4t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet50 is only one of many different CNN model architectures. Determining the best model architecture is an important part of the model selection process, and in practice, it's best to try out and compare different model architectures. A detailed description of pretrained CNN models supported in Pytorch can be found here: https://pytorch.org/vision/0.18/models.html\n",
    "\n",
    "\n",
    "\u2b50 **YOUR TURN!:** Try a different pre-trained CNN model from Pytorch, based on the available models found [here](https://pytorch.org/vision/0.18/models.html). How well does it perform compared to ResNet50?\n",
    "\n",
    "Make sure to modify the last layer for classification to match the number of classes (Hint: the number of classes = `len(dataset.classes)`)."
   ],
   "metadata": {
    "id": "RTtAvwK9VO3W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = ### YOUR CODE HERE ###\n",
    "\n",
    "# Modify the final layer for classification\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ],
   "metadata": {
    "id": "lie0tTMF-KdE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "WiuNJFl2-MOY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "ET8xcMK0WEeJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we choose EfficientNet-B0 as the model to train and evaluate. More information on EfficientNets can be found [here](https://arxiv.org/pdf/1905.11946)."
   ],
   "metadata": {
    "id": "pNiMyUN3PZ9g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the final layer for classification\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
    "\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ],
   "metadata": {
    "id": "I7Ybi3eEWF_B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "28367d5c-a3b1-4a19-b275-9ce5ad8a09cb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Commence training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "JwyC5pyLXRUL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a25fd92300254708bbef027228c28687",
      "c08bce0a86ab4f2da9b6310114c92ff7",
      "09a8f8281cce4acca1c5580ce8645a22",
      "c93f11e8b26b44499dba7f263e7b9ca0",
      "c8cfff4eb1cf44c3aeaa27c99843bce6",
      "3d3182e3ddfa46eb8c0edf887ee17875",
      "49437db4a42b4425ac628e5cb4d93890",
      "f0c60dc8e0244e42ae6b9ca202cfb09c",
      "d2dd635128c94c668ab4026b237ca756",
      "5694a708d1ef41bd81012deca8d13db7",
      "9f7bf5aa15cf400883e6053cb9d2d667",
      "bc7a4f35b70149718e1da40a81e096d8",
      "f6894ef0d7ba46a187f008dd9350974f",
      "3966c7b5cb0d449981c511ad0bb2a6b9",
      "f8ecb444a6b84e3a9db8f630c32d6c04",
      "a39b8325bd5149daa31ea7df0cac9bcd",
      "4476e7cf4dc041589778892c432f1267",
      "c579bf347ba94c6aa987e43a1dfb2cd3",
      "6114377078cf46c2a1b728dee091791b",
      "56cb96b6b1f2462c8b665d63f6214f82",
      "9daf143e94e8473b8840ce5a1fcae7e5",
      "cb9ac7ff16aa42a9aa5b3138b84994f5",
      "91e5dae80972473aad096362f7d30388",
      "addf5dddb4e84a5f94ffba71fef1e3a8",
      "e705bbf20c364e4fa025cfba0cb23bbb",
      "2ec9465482fb44e7a618cd7af46fb745",
      "75e9a50ee94b43eab3947baac531360f",
      "fdf6b872b49f46c9867f5503b21e8292",
      "270e8632218d484b96ff14b70a516069",
      "98dbb0cb0e2f4dc9830fa1b014edce34",
      "69c60d7f152d4edcb2375a9d79e098d6",
      "cf8d23f09d56402b8e68f44760c0dca2",
      "382c62ed98e941a09771b4e69f2a0c97",
      "a9978f35f8324818bbbfabaf0c877d35",
      "5e73849bb0af4be280c4cea75bc28f3b",
      "fc58a11e2125462abc498aa6cdabf110",
      "04d07ea0187d4c0398890adba327cd9f",
      "84a0f655303440f3bf72c46fbd1a1a99",
      "86659d458fec4a1c9f27945bafd1ae31",
      "0c3cf6ff126e4bc1adaf41ca87397423",
      "94272e11232242408b55c9a12ede5abf",
      "db8d6f72518c4050ac0dee744a1e39df",
      "f6e4700d6ecd4dacad699f37f07d6eb4",
      "f6fef30be96c4a87b978396120c97684",
      "37d72a2e40eb48e2b1c971f6f9bc8c34",
      "ebe15946bf674c629ddb4ed5cd2a776e",
      "84aaabdc4f64412e913b54716f497b0a",
      "5519fc4a2af14fe5928001061034003a",
      "36b7c100bd964ad789917e903388d8db",
      "d747a77a0e654f36b6abe6f24375de15",
      "d37a088c99ec434789e079e199f9314f",
      "148ab59bf847444a9fb5bf07c1903415",
      "3954b64dca6e40298041715c0e1708de",
      "cc22ef2c44d448e28f84324f72824911",
      "1f6f94534a4843b8967968e455b61231",
      "9961251a25eb47ef893e2b0ae33a175f",
      "6e816eb3e616434480f803f22769c0d6",
      "3e8f0f2884894ace88d71e9fc5cf2469",
      "70d279445c384d08b9add97fac8d298e",
      "8991972280c84aeab030668482ec4d7a",
      "4f70842c45c8421a89e22dbd886e3d46",
      "7a5cba2d0ab64ca483170deab8596608",
      "b984d071227a42af8cabe929892f0eec",
      "17696469ac634a47adaf6a8cc4647722",
      "e8da2b3b24da4e11bcc178acbccce9c5",
      "b6dbc8bcf63a43f89016270901bd51f3",
      "96ecb2e77b3f4486aa6bd5302f4788d9",
      "571bf57870324878a3a24e69a9519ee8",
      "87ee8e8f6aa64ac49ac9765226f8c681",
      "a07a15e3acda4e4fa168885127234f57",
      "7ac5e25e98d6488193d19bce29775318",
      "d33283ca0b7c4b9d9a1ad3fe7965c111",
      "fbd2a65322b2480e924cabc724191f65",
      "29b8aaf5950b485db25f67df1bbfea05",
      "870daf0b3392446180f7c5138a70aa91",
      "9486470b58774c89aa6841b75ac42e8c",
      "aab67eb1fe084f7e8c4313b2cbadfd9c",
      "e9a143a7c1b04cf7b0630a2f8bf05bbf",
      "7bb5ef8e67a34bc29de978bd13df3d27",
      "aa496023f65943799987673f73ef8155",
      "8f9ec939189c47489ea3294e52d74aa3",
      "55ef0df4d6074663938d8367c672da9c",
      "4841bf9cbafe44278e56d7f3c237a0e0",
      "38625f2fcb1044079f52517faa33c5b1",
      "6a4f773867c44cada023abf1250a0fcd",
      "a98392f779cd4e35a49809aa6842198c",
      "a5f55ee3ac874ae396426a3bc7b32aad",
      "1eb9d62d4f0b45f5836afe5daabb6874",
      "9b28da9f046b4caa813d0c2b79009ffe",
      "565896f481f14cc4930540804405d2ed",
      "2d96b4ae57164975a94493dfd150a142",
      "f4406f2f00364fd1a45f5a3fd9d17942",
      "88b9ddc38f9a4756ae0dec01392e4291",
      "3d8e043359af4d16a2dffe24ee966628",
      "5b73ac3be92a4f37a1e720b8fd9c0f3c",
      "464ccd7950814c099434a9f9d472250d",
      "a277eafa413646d0a37290afba0cdc86",
      "d86df729345640d6a827924bf20c2fe4",
      "09b30adcb9574f6c8a13b816a53f41a4",
      "4c23546d1d1449158ad9d3783ddc703b",
      "7a1ac215fe63448e87f48ed07bf27116",
      "028d3489661448b1b197459e90b728bd",
      "0f59d548e9a24dd5ad4cdc7b258507bf",
      "c3a48b73bb404b118ecddab857864afb",
      "ae9bf628ee444558a230115308338db6",
      "bfb3b6f4fe7b4d8cbd107254658bc6a3",
      "da52d4d574bc44ca85e19801365219c0",
      "4eb6130c33af4901b9e7e2c1c6534cee",
      "ab74d04c6d8f44a0a3a098eddb53dd88",
      "d504ff967886466dbd3cc15e3911d5b7",
      "358ed12999804679b386f1f81bc5e856",
      "1552ad826ae3453a802903fccd73f3c2",
      "4fda5145cde14022b2a600483e33ae66",
      "9dfe19539a4344d1b9b389ecd9fd5dee",
      "150317aa2572417aa656fb52b729bad8",
      "61186d8f815843d48aebbb1d2109f188",
      "63fcb8a886ee46be94185f396aab1ff8",
      "77b8a1fc2d1a458a9cfbb7045a01d010",
      "2d3bb33fad1644ddaa4d5caa8a0a7a0e",
      "1caaa3ae263b46cb96fcfad1e34cb6cc",
      "7e7e81ddaefc4597919dac1aaf80b6cb",
      "7e69ee772d534a1788463dedf88c9b7d",
      "014d1e82fe3f4983bb93fc91f6ea3807",
      "9caa61c16f144b5da0aaebfeb932ef8a",
      "f48215b18cda4f6789cac34138bbc94f",
      "a725aa83a8cc47a78a4ed66486902d6a",
      "df0419ee90784f2facea5ef85e077cc8",
      "d7ec8dd8635345d8b2dcd64278fc982b",
      "b31d8b61a0384c5f9e441ddbe04d2430",
      "193f4b367eb64897acaf5019faa37848",
      "337eff0b0be64807ba560829e2df8ee8",
      "7e24bd69fe4c44e2a0e6c525f0b68429",
      "c751f63bed164606be731a688596a3c0",
      "f1d1c546fe404dad9ef1f613b1ec400e",
      "37a7cdef50aa43128737ebb7307516c3",
      "db78bf8c3c9c41a2baec151e9260223e",
      "923f1d8c3a9d4814b2e4a0f832b17ca4",
      "a03550aa35f84a028f35dcb911682a02",
      "a6d7873f7eab4cef86206860c0734347",
      "496e47789f5e489990391e3d9d00c29f",
      "5f1dcf1fc5d24e81aefe4f1b2a60a17c",
      "02ad75f6954140a6974de43e8dea28fa",
      "4d5ec4532bda44a38db3b4b1aa457067",
      "ece15485f6994c2daa07f6aacdd83b89",
      "c800ab645c4246588d4f03878294cd3a",
      "b17f1917b67c41c8b3a87be12c6a62f0",
      "61b301f1bc57476f9fe8b0834d0a4131",
      "aab472298002403084e6cc03ab461d99",
      "139e63d45cbe4a818176784aec5e5def",
      "24fdc3c6395b4087bdb5d312a58a0e8c",
      "faac6651934d44f1890a973323beeb30",
      "38943583bc0243478652f6aba524426e",
      "1461ddc3faf147438470a6dba2d9d9b2",
      "dae38bfcab41458ba2d35943e913fabd",
      "e7196022039e4281addf9b70bb4a7582",
      "6d6294c84d5940fca74ae3dca1e28ee7",
      "56e9dec2ffe9498c9521fae35e349c9f",
      "73a0e82569114304bd5bf33152193dff",
      "136f728706a840faaa8631f357186bcb",
      "df61c5623cf94db2a974ce2156e8afe9",
      "6acf34ad4bd947fc92ccb1d305f262b3",
      "d430026ecbe6479595c16db594a26b50",
      "2367cb9baf23413596f4f74e96078591",
      "85bf40ea6742464ea425b49d432bf7e4",
      "f11f6463d0af4ed891724042567a7038",
      "e604a814447a45fca45a415ccaa03403",
      "b2eac03ef7a64342abf1c472536ba752",
      "3f44ffac216f40a1a699ced5711d6991",
      "2bbb5ed7fab347f5ae41f8eafd7c2571",
      "1d08736840604b3c8fbb8a0eba24ccec",
      "f4d3d784a05949dea0079de1065a841c",
      "be7d14d365d644e4a7686260e0571675",
      "f47e463a3b0e46d99ba936a45b3aeaad",
      "7570b0e9948943f2b9b18741877619ec",
      "5b8b492a8a3e455795326cd8f9b32dab",
      "22f7f5d7bbf9476d8d35e698a5cca1e5",
      "b161f93fc15f42b4a59f1232ab75d7b1",
      "0eebb855a7c34f5a9ce760b9c0d68672",
      "e76c8e894d1246288140a93b3c559526",
      "b0cfdfa431ff4c3eb71715e5185bbe7a",
      "033535e818134b1ab3abcf14c1d77b25",
      "9ca75f6fae0a4615aa46c50edbcfcaac",
      "395b56afeb60495eae694c90b2406138",
      "2f1d5de89c824f23a8985c77684a15f3",
      "cfda555b0f34462a8ec395194e97ae44",
      "74b52bacc97f41dfb2018b03e8e6a580",
      "d845e34eaccd4e68a1dcdd9864022bcc",
      "63d14d2bd7d9425890eca4cb3bfcd659",
      "d05caf107dcd43b4bc48c1cc003e0b26",
      "90aa3f098b5847d39725550e28e20a9a",
      "0e1b92f974d54ae099d8799f117b6e8c",
      "f20ac5dcc6b04c3fa2aaff047bce92bf",
      "390bd97749e247cdb52fbe04b5206132",
      "f1795c5cb01c42d394242f063cf1a000",
      "e7a1175f0245440bae76a713d345bdc6",
      "8f34dbc5b60544b8b4d15f67eff29e0d",
      "596fa5f797864495992bd573b69d036f",
      "2fca072868c04cc2920b0eebf2a0474f",
      "145c27e7c8ad443bad705f1bff532f25",
      "77cf748e32e54ed1b456d16ebbe6cbac",
      "b35ce7dba3b24cf0845ea9dd15ff92d0",
      "f07c5a2dc93d48129da29e23706d1c8e",
      "5703373258344b66b93e0f9e7fffa753",
      "817fa882067f415e8a67860f25cc4b18",
      "e6a22b15310145f8b5dde08918c7522e",
      "d8de1b70a93e479f8f251d8b1b44348a",
      "5d51c9677d85467b804529730c3d7f48",
      "1f34a97d0be1418298e87f81fd31d3ee",
      "7a0674fcc6054cf99c54b3741f4f3061",
      "3f9a18bfa1414fdaa145140313d535f3",
      "b111f12ea29a498dadb11dd72a4acabb",
      "cd64b2ec7b9c4a9ebbb8ba6b064c2ef7",
      "77aa126527734ea1a500cfba273a7306",
      "6fad9629553d413c99ebe12ce741ae12",
      "ad2ebc1b64884e91b52e3ef1f8ed3b91",
      "b9132734d9e44cd486dccdaa60026dcc",
      "c6541e23543c42ca8944aa41cbc7f400",
      "c8e88b8adbdf4acc8f2abe1ab334350a",
      "97edc9a26f9d4482902858f317d08116",
      "31bb822677f5405c9882a381e469cbd5"
     ]
    },
    "outputId": "73132d8c-5fae-4373-a955-4b3ade8e9810"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Experiment with Satellite Image-pretrained CNN models.\n",
    "\n",
    "Practitioners have traditionally relied on Imagenet-pretrained models for transfer learning tasks, even for Earth Observation data. However, with the increasing availability of remote sensing data, we're now seeing more and more large-scale remote sensing datasets being curated for model pre-training, such as [SSL4EO-S12](https://github.com/zhu-xlab/SSL4EO-S12) by Wang et al. Models trained on these datasets utilize unsupervised/self-supervised learning techniques, leveraging the large amounts of raw, unlabeled remote sensing data for model pretraining.\n",
    "\n",
    "These pretrained weights are made accessible via the [Torchgeo library](https://github.com/microsoft/torchgeo). More information on available pretrained weights can be found here: https://torchgeo.readthedocs.io/en/stable/api/models.html#pretrained-weights"
   ],
   "metadata": {
    "id": "L4pp77orVXOm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\u2b50 **YOUR TURN!:** Using torchgeo, load a pretrained Sentinel-2 pretrained ResNet50 model and finetune the model by updating **all weights**. How does this model compare against the Imagenet-pretrained ResNet50 model?"
   ],
   "metadata": {
    "id": "1D07TUK9VmRp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q timm\n",
    "!pip install -q torchgeo"
   ],
   "metadata": {
    "id": "5SLg99HeWC7I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import timm\n",
    "from torchgeo.models import ResNet50_Weights\n",
    "\n",
    "model = ### YOUR CODE HERE ###\n",
    "\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ],
   "metadata": {
    "id": "KdMt3TCFZI_c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Commence training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "eTUvrzt8cJm8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "KLhzjCifTcW5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import timm\n",
    "from torchgeo.models import ResNet50_Weights\n",
    "\n",
    "weights = ResNet50_Weights.SENTINEL2_RGB_MOCO\n",
    "model = timm.create_model(\n",
    "    \"resnet50\", in_chans=weights.meta[\"in_chans\"],\n",
    "    num_classes=len(dataset.classes)\n",
    ")\n",
    "model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
    "\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ],
   "metadata": {
    "id": "HFRKhevPVZoF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ca3c8f0a-c1fa-4760-9a3b-8b724366f521"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Commence training\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
   ],
   "metadata": {
    "id": "6rIIhjK2Yy3F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f09abd21f8cc46c78eeef877cb2e6c65",
      "b9308775e28b4255894389e2180da760",
      "4d329631ca674569bfb3c89864eb641e",
      "e456b637690c40d78eea2d459220bdc0",
      "fe5c16256ce84c938a716e8188f0ff80",
      "28b6848b07b240c4a01773188bd98ef5",
      "e5df6b21b8bd4fe093cee6f037792914",
      "92ec826d07744312848840e5fc1e5db5",
      "50ef58bc80eb4fbe9420123b587c65af",
      "5a002c1b25d84759b56c16b932a9ed63",
      "fa785007ac544197b416a3041a273d77",
      "e85443f173324cf18b1ccc75467de504",
      "c96c3c6f3a8b458db042e639d345ce8b",
      "596e05ec3c834ee5996f43e83652cf78",
      "2cdf243b8aaa411ba2d0edbb988f13c3",
      "ef82797ac6ea42d5bda063b7c873e285",
      "4a53b2c68964403c89e1ea355bc4aed1",
      "07c652f64bd543e7a3ca491b207048f5",
      "0d4bcbbd776341feaee91b9fcae47e91",
      "2d3e751486b441a4820b69562033efac",
      "4320f71cd9ec47c1b4ede06a37b869dd",
      "25063d1e31d042d788f905f18530fccb",
      "2c1b8a26c7da461791bc18fb7bbeaf1c",
      "4dca9d4344f84ce7a8ef631649409a2c",
      "30e8232cfb7a4a139149b6a20c15909d",
      "a89575c0c85149e1abd1cb4b5c445356",
      "7c3326c536364b38a6d476e49df1a133",
      "6b6919b335ef400aa613e948fcffb803",
      "f444bde5a78040a38a540a0de630d289",
      "d8f69e3d274d4f7c879bb4c6c10ce2c7",
      "79cfd35cde9a438fa5a181cdc2a178a8",
      "cee30ade530a4241958b33b71c963b96",
      "7eb72a07a4364924ae3be4a2bfc495a9",
      "ee449758964540649c06462e9ed97bdb",
      "1ec27e1ba67b4725b1a4e7f93cf681b3",
      "e2eefd20f6234bf8b1bbdbc5dd4f6822",
      "4ec8d1cb12bc46ab82567f72d9d0ec65",
      "91850e8ec4b54d7691973eebbab5a497",
      "463cb0acca6947209d8ab97bcf5a3c08",
      "4750356185634ab49cddef11756ed2e7",
      "f7f55ac72a3d4bc8b890440faa617448",
      "c17c3e3803c84e7d9067f706805ae869",
      "9994c963c245480ca9c5a1cc710023b0",
      "50cacd476c424307a496c7552264e002",
      "29beab5bbfe64782a1bfaa990332551e",
      "31649e2b0b7a486f9f7f2d9305256711",
      "3be6d8208439494d8e418b10fed9d71b",
      "6b7c9bcabb0c479f94cb0e980d539784",
      "3404d40cd8884cd9ad00302282358d5e",
      "95f36d72efd34b909d7a1575479bc1bd",
      "56688d23d0f94accac100bf0b0143fe0",
      "1f36e45122df48829a8b78f3670c54e3",
      "145e6393360940659a8f8b55552798ce",
      "6b11dc87b0af4b93b75f9cb3c425a663",
      "51b5c41d3a3b4b3aa6dff1382a1e4556",
      "d2ba875f77a44a9f851bdffbb79e125e",
      "9ba63edd48a641df9f9419f9d94797af",
      "7781d70fa4d54bee80a6ef4e1b1ebdbf",
      "12ffd0a9306240088538e5f4992d9920",
      "88d829235a5149a0a361c8f71e3216c2",
      "0d0cf6f4879a44d499d6fb81fc9cd5d2",
      "d3741b9dc4b84d56a555052b0fd56314",
      "92541fd363f640dfb1bb2c64f65475bc",
      "abc83025ba154e95b8aa1626b585f4e7",
      "edff2b62243c43d890957ed797ddde34",
      "60afe6dcffd24880b5fd4d769a3e7da3",
      "e68516800f2d46b2b22437074f29cae9",
      "811597fabfc146deb68ae8e052b6a07d",
      "ab1aa1f0ede4431385e240f2804060c9",
      "d247af733cd84e99ab770c1f688c02be",
      "f555e5a3cbb94e97aa90bab901bb8781",
      "85b9aff9229d46b08f1e0207829d15c2",
      "b71e8f1517014fa4b7dcc26cd1ed9249",
      "bf2772dc8e0e4e728cc59d1bc917baf1",
      "bdc0d801cca94e26adf75706727873a1",
      "a9135f7d6e1842edb2e2fb3f3f774d9c",
      "aa5230a1ee534d42b39951387ef4b8f7",
      "ee82b3f70bf842bc88f890a148038c44",
      "4cd827d6558d44519bbd21ec421e1b7d",
      "00acc3212c2e41ac8d870481052b932e",
      "062091f8fc73478e9bc7a5c568ae8875",
      "4c1f99b9c0ad45a28d4d9dddd6cb9ecc",
      "4b90960339a24dcc99040e547911e0c8",
      "0896054174f74e4ba931f751f6cfab41",
      "7e337248afb94cd99b8c54dc09cc2756",
      "098872e11c2b4caeb3926c23c80894c7",
      "f71b894d2c14426180af516fea24f7e7",
      "9b1677fb63ab49958d7a32fe55c50dcf",
      "5b59f7da585d40148ebb92dd959ef9b7",
      "f9917d68d7224b2d84ed622890c216fe",
      "9e3f209ddacc4846beed13e9f13fad63",
      "a67c0ee17fbf4679a8aa9cdfb41e1086",
      "6c858075c1444ffaace097134fd68c50",
      "eae1b2c156bb4933b3572801fe016057",
      "9546f3ce03c34f71aa6bda1ba39b4ff7",
      "09ef13e102874f6b984e6ff18f2ca5a6",
      "78bc15ebe8dd496eaecdd2e95ee8dcd3",
      "8a1f75400fc84e2984fcf650c28a1579",
      "66ed7dba81ed43c3bd2b3ad6f979fb57",
      "4a7b63fd5d2e4f988ad023ac538769bf",
      "de870414e70a4e17b7422b2dba1551ed",
      "fae77d36c7504626966ed69ce2f1397b",
      "35b5b9a77ce8449991835c61500657d0",
      "ea3b0fffcdbc410e99c2caa84c71adb1",
      "43e777ed3aa342f0b4ebb3bf8d13f9fe",
      "962fddac69ee45d08bcbdb1c86bc26b2",
      "611568725f26482987678bbc85e75094",
      "2d86356d8f4b4dc0bbe3c17310713e40",
      "336b75fca86345178d401889fdf86dba",
      "9f6e6a0ce63343eb9d49e48d8039c94d",
      "134544b14a88435697dce379991007a6",
      "7d70b7e42dc04c70998fb0d49c50300a",
      "763bb9268afc462ab1a689a9d1a7f77e",
      "4fa4e7787e584e85a11e31b18715499a",
      "5e9d49a037af4387af2865583bb36fdc",
      "5264363ec48a4e778a7f9cf042f91863",
      "acc07d8cd89c493e96e52076e4c5f8d2",
      "4cbc76ea877a42fe8e19c8368c9fc5cc",
      "1b689fbbf37b425c87103b883b6865a2",
      "18b6fd26fe0e45629b52bf1e6dda8713",
      "d74134a7922043e0abae72a82c447d93",
      "c33975816fc945dc8983c6d0e8393560",
      "d6547039d2dd40ae9b4b0df740f464c1",
      "d10df86a8aa348609d816ed0beebfb5e",
      "caf2e30befcf4445a83afc0d85d37d0e",
      "37fbfc8b47a643248436140d464b2e6d",
      "dccb05fcb7bf4446b25cb589fafedeb7",
      "a5d3d2a92fd54d03affa0a2912032777",
      "5f5003e4434b4e23a7e6db95d90deef5",
      "1ce5b1483cca48ab8c491f1c7f6a25bd",
      "633efdcdc37e4c418700c21ff86dc90e",
      "6d666e507313406fb2a36ca270f6b8e1",
      "9acf208f13424cdf8d2350c762df8fd3",
      "9cba7f5845b845ab86386de0785bae3a",
      "db308d8c5b0d477bbb36fe516ae89307",
      "0601c28b188846bdbc2cd6a6f1285468",
      "2c2e53e97e2c41e18fa969b85116f5fd",
      "7059110464d44749ba0010a7e93c8b32",
      "6c4227cb6d214322b741db287761d820",
      "72814bcdd1954ebd8ea0056a3663f54d",
      "ff9e4bb9ddc64abab6a2ada878ad5939",
      "84082a4648ab47168ec9acb91dccb1e7",
      "a14b050030f84f2da41fdbf85b957e09",
      "e19b247cf75f48ea93a649ea7e5a07ea",
      "c8fc2c37b56c42e789205dfdb7725bb0",
      "2e9d8d977da042aa9d25d38de13b85d6",
      "0ed77ef92c1b4b44bf4b7c4b52e99451",
      "fc59cd35bd634a78904a3011f33a5b1e",
      "194a6330eb2c494e86532d26f7c5c65f",
      "25494a5b9ed34caf93af20eddd79d36c",
      "64797ba209d44182aeba4c45a3495f44",
      "ccd2e7e3c1fc47f0819b2c89c60f4690",
      "6b015382a4754966827631369c036e55",
      "d974ccb2aade409b9013beee6a449653",
      "d194fa7456c1490d8801658d9f6c0173",
      "e418cc02471c429297721b8d4fb4aacb",
      "e8f39b67a0564714ac635de36b1a5713",
      "8e079362211a46dab8256d0790369510",
      "939738242d9e41159b8f442080ebd999",
      "3395d344ba4e4939a75e70ebe586d532",
      "357c70f1470f4e3cb89b84a8d0856110",
      "ddc21f06b4f04f499174f316d641325d",
      "0874ff9404bf42e4994975efa9ff8fba",
      "c4e5ee49ad08493e8f8066bcca405fb4",
      "a6791c1a5c1d4e4abb8847a2f6f032c1",
      "343562e161354efe89a150bb9b6f0efa",
      "9de9b581f2f74bb5b2857d85ad7ded57",
      "b6e1a8ebc7f248b1838a8c60c105d278",
      "76ec01da97024c5e8460c88aedfb96c1",
      "994f28f055854453a33c306a72282439",
      "66931b1cb1b54b468ff8c3cc29d5c114",
      "d1d2b6a9bfd04eb6b788b0f0ce767133",
      "106e1a5b292147aab3acd87b701d64e4",
      "038b72cd000d4f2dafddcf12e6d8f03e",
      "79aa1b3e31cf4f7c95930c5b464bf350",
      "e7cb1ab5c61a423fbc8aabe185cd00ca",
      "c37d5e2b0f844fc0b6c9388ab7702dc4",
      "38ee2928480143a2a4d23916e410e2aa",
      "36c161254bef45d09e3e5911f973a004",
      "27efed6a19524426b9881f02387dfddb",
      "aecd6e6d7ade48ce9f3d1bbee3eb34f4",
      "a060d3573aef443cae7bdc82804194ed",
      "ad232adc39234da0ab03198c1963d2ac",
      "6161a83014f54bc1b2bd00371f42a63d",
      "888ce9ea46ef4de887631b5c4b9875c2",
      "def656424dd34f5fbeef8d9b4f4f51ce",
      "3caa9ecd96fd494e9fba5623bcd31814",
      "ce3d66f1b75e4ba49571df91d39dd0ec",
      "42d2859c15204f8b8577f997615de32b",
      "c70d1441cd5b4bd598007cf07936cecf",
      "917e335580ea4b308e6e7d6c9a57f89b",
      "4bf6460347664993bf8e2d3a4ebc4a2f",
      "4ac3b0f5e48741c9b659537b887a11ac",
      "5baeab8d3b9846febd9f42b093c0a106",
      "4bf597b6916c48c99a5ee42c55b025ca",
      "29ef5d793ab1424592804577e8dfa47b",
      "03c3914e6b4d426783deeee0236d4c84",
      "5c4c91b017374912ba961580584cafb8",
      "c500edbf4d194f47b7d30bc457a37b96",
      "b52f9620951a489bb3632b87caa08c4d",
      "18d5eb8cc6db4eadadbfc7ff60b5af08",
      "b61be26af39840cda1e610ae56e27d36",
      "50dd17dacaea465f93d7e508dd938036",
      "84d31ba9ac0d415d8fe88a68782fdb81",
      "2f8872c2eada4f55863c85d68e72cf66",
      "9b06a5330bc843aca7eab7bc4aa4f112",
      "31a895cfad9a4dbfb33dfa68ed0ac4d6",
      "0e1b7d78a8df4bbca9aaf3bff93d61bd",
      "1a99bdfffad946d491a2d5f45207f4bd",
      "4c147a57b5424182be83070b300efbdc",
      "1ed6c54c69114b6dbed06746812041fe",
      "c20e822edcaa43548181d3f85250eda3",
      "2708fa4cc5aa4ae49815e9cf0f50bf6a",
      "e0c112955d4e4f1dacd1bfdc2c32fd8c",
      "fb5ccd0d425c4e2d983af6d80c2ac85b",
      "97cecb2593474a2fa83ef52dc07c03cd",
      "68db0eb92288462190ea751806e95105",
      "2f4932edb78841a398143555a4afa467",
      "42ab714b95834cff8c1b94ec6e7ab5d7",
      "d5ba74b2060942d4aab2407c805a591b"
     ]
    },
    "outputId": "90df0ef4-8287-47dd-aca9-285a21f4dd28"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congrats on making it this far! Go to the next section, [Part 2: Automating Land Use and Land Cover Mapping using Python.](https://colab.research.google.com/drive/13I1wZT7thBlNdGA1tFQrK1MlRhMZMnpM?usp=sharing)."
   ],
   "metadata": {
    "id": "fObmOlAL0o6v"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYk0TXtZ0G4Y"
   },
   "source": [
    "# References\n",
    "- Helber, P., Bischke, B., Dengel, A., & Borth, D. (2019). Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2217-2226.\n",
    "- Wang, Y., Braham, N. A. A., Xiong, Z., Liu, C., Albrecht, C. M., & Zhu, X. X. (2023). SSL4EO-S12: A large-scale multimodal, multitemporal dataset for self-supervised learning in Earth observation [Software and Data Sets]. IEEE Geoscience and Remote Sensing Magazine, 11(3), 98-106."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automating Land Use and Land Cover Mapping (Uganda)\n",
    "\n",
    "This section adapts the automation workflow for Uganda and the land use classes listed in the project brief.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/climatechange-ai-tutorials/lulc-classification/blob/main/land_use_land_cover_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_UfmKI-KF3t"
   },
   "source": [
    "# Automating Land Use and Land Cover Mapping for Uganda\n",
    "\n",
    "This section adapts the automation workflow to Uganda and the target land use classes required for the final vector map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaiH9CgIoHp-"
   },
   "source": [
    "# Introduction to Geospatial Data\n",
    "\n",
    "This tutorial covers an introduction to geospatial data processing using Python. Our aim is to introduce basic concepts and commonly used tools to manipulate, analyze, and visualize geospatial data. Our targeted audience are those who are new to Python as a tool for geospatial data analysis, as well as beginners in geospatial data analysis and are looking for tools to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvDhYa_-sdet"
   },
   "source": [
    "## Spatial Data Types\n",
    "Spatial data observations focus on *location*. There are two main types of spatial data:\n",
    "- **Vector data** - are basically points, lines, and polygons. Each vector object can consist of one or more XY coordinate locations. Vectors can be used to represent, for example, locations of places (e.g. schools, hospitals), roads, or country boundaries. Vector objects can be stored using spatial data formats such as GeoJSON (.geojson), GeoPackage (.gpkg), Shapefile (.shp).\n",
    "- **Raster data** - are composed of a grid of pixels. Examples include multispectral satellite images, nighttime luminosity maps, and digital elevation maps. Each pixel represents a value or class, e.g. red, green, blue values in satellite images; night light intensity in NTL maps; height in elevation maps. Raster data are commonly stored as GeoTIFFs (.tiff).\n",
    "\n",
    "To learn more about vectors and raster, [see this reference](https://gisgeography.com/spatial-data-types-vector-raster/).\n",
    "\n",
    "<img src=\"https://slideplayer.com/slide/6229417/20/images/10/Spatial+data%3A+Vector+vs+Raster.jpg=100x100\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po8wZBRtskHi"
   },
   "source": [
    "## Coordinate Reference Systems\n",
    "Map projections are 2D representations of the earth on a flat surface. But because the earth is spheroidal, there is no single most accurate way to represent the earth in two dimensions, resulting in a number of coordinate systems that serve different purposes (recommended watching: [\"Why all world maps are wrong\" by Vox](https://www.youtube.com/watch?v=kIID5FDi2JQ&ab_channel=Vox)).\n",
    "\n",
    "**Coordinate reference systems** (CRS) provide a method for defining real-world locations in geographic space. These systems determine not just the coordinate locations of objects but also how your map looks and how distance is calculated.\n",
    "\n",
    "Geospatial data - whether vector and raster - is always accompanied by CRS information. Two common coordinate systems are EPSG:3857 (Web Mercator) and EPSG:4326 (WGS 84) - in this tutorial, we'll be using the latter CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "W2eF7_2AsAFq",
    "outputId": "ca19ea82-a218-477f-9284-a1d6d068ec6b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('kIID5FDi2JQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xHy-Nb1slqX"
   },
   "source": [
    "## Geospatial Data Processing Tools\n",
    "We introduce the following geospatial analysis tools and Python packages:\n",
    "\n",
    "- [**Google Earth Engine**](https://earthengine.google.com/) - a public data archive of petabytes of historical satellite imagery and geospatial datasets. In this tutorial, we will use the [Python Earth Engine API](https://developers.google.com/earth-engine/#api) to access Sentinel-2 RGB images. Note that you will need sign up for access to Google Earth Engine at https://code.earthengine.google.com/.\n",
    "- [**GeoPandas**](https://geopandas.org/) - Extends the functionalities of pandas to add support for geographic data and geospatial analysis.\n",
    "- [**Rasterio**](https://rasterio.readthedocs.io/en/latest/) - Raster data such as satellite images are often stored using the GeoTIFF format. Rasterio allows you to read and write these formats and perform advanced geospatial operations on these datasets.  \n",
    "- [**Folium**](https://python-visualization.github.io/folium/) - Allows you to visualize geospatial data on an interactive leaflet map.\n",
    "\n",
    "For more geospatial analysis tools, see this [comprehensive list of Python packages](https://github.com/giswqs/python-geospatial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CDbvN6Zoefb"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN_NZz2XtgKm"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install --upgrade folium\n",
    "!apt install libspatialindex-dev\n",
    "!pip -q install rtree\n",
    "!pip -q install geopandas\n",
    "!pip -q install geojson\n",
    "!pip -q install geemap==0.17.3\n",
    "!pip -q uninstall tornado -y\n",
    "!yes | pip install tornado==5.1.0\n",
    "!pip -q install rasterio\n",
    "!pip -q install tqdm\n",
    "!pip -q install eeconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew3_MrrGslr4"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Geospatial processing packages\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "\n",
    "import shapely\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import rasterio.mask\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Mapping and plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "import ee\n",
    "import eeconvert as eec\n",
    "import geemap\n",
    "import geemap.eefolium as emap\n",
    "import folium\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7UQeErEItTQ"
   },
   "source": [
    "### Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQvQCFgfIsoo",
    "outputId": "1c6e1ed0-70ff-419c-b573-c33415e915e8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8JfmJxa043J"
   },
   "source": [
    "### Authenticate Google Earth Engine\n",
    "Make sure you have signed up for access to Google Earth Engine at https://signup.earthengine.google.com/#!/. Once your request has been approved, you should be able to access Google Earth Engine at https://code.earthengine.google.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5nXj4Fo03Wr"
   },
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize(project=\"<ENTER PROJECT NAME>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4hf8Z5CwSOE"
   },
   "source": [
    "In this example, we choose **Uganda** as our area of interest. We set the ISO code to \"UGA\" and ADM to \"ADM2\" so the query returns district boundaries. ",
    "In the following cell, we send a request for the Uganda admin boundaries, save the result as a GeoJSON file, and read the file using GeoPandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh_8NeAcs3qS",
    "outputId": "0b4de411-629b-4755-8c11-60cecaedd57c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "ISO = 'UGA'  # ISO code for Uganda\n",
    "ADM = 'ADM2'  # District boundary level (change to ADM1 for regions if needed)\n",
    "# Query geoBoundaries\n",
    "url = f\"https://www.geoboundaries.org/api/current/gbOpen/{ISO}/{ADM}\"\n",
    "r = requests.get(url)\n",
    "dl_url = r.json()[\"gjDownloadURL\"]\n",
    "geoboundary = gpd.read_file(dl_url)\n",
    "geoboundary.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r_o242a1tai"
   },
   "source": [
    "In this example, we visualize the adminstrative boundary for district **the selected Uganda district** using the GeoPandas `.plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hW8zZOy9t613",
    "outputId": "42087dce-25cf-4227-a62a-6d55c0648c40",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    }
   },
   "outputs": [],
   "source": [
    "shape_name = 'Kampala'  # Update to the district you want to map\n",
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "geoboundary[geoboundary.shapeName == shape_name].plot('shapeName', legend=True, ax=ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMBvT_V355PW"
   },
   "source": [
    "<a name=\"sentinel-2\"></a>\n",
    "# Generate Sentinel-2 Satellite Images\n",
    "Sentinel-2 is an Earth observation mission from the Copernicus Programme that provides global multispectral imagery every 10 days (2015 - present) at 10 m resolution (i.e. the length of one side of a pixel is equal to 10 meters).\n",
    "\n",
    "Images are typically composed of 3 channels or bands: red, green, and blue. Sentinel-2, on the other hand, is able to capture 13 spectral bands:\n",
    "- 4 bands at 10 meter: blue, green, red, and near-infrared\n",
    "- 6 bands at 20 meter: for vegetation characterization and for applications such as snow/ice/cloud detection or vegetation moisture stress assessment.\n",
    "- 3 bands at 60 meter: mainly for cloud screening and atmospheric corrections\n",
    "\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;<img src=\"https://www.researchgate.net/profile/Gordana_Jovanovska_Kaplan/publication/314119510/figure/tbl1/AS:670480428195846@1536866399263/Sentinel-2-band-characteristics.png\" width=\"400\"/>\n",
    "\n",
    "For simplicity, we only used the Red, Green, and Blue bands for LULC classification in this tutorial. However, multispectral data contains rich information that can be useful for a number of applications including crop yield estimation, vegetation health monitoring, built-up area expansion analysis, informal settlement detection, and so much more. We highly encourage you to explore the full potential of Sentinel-2 satellite imagery for climate-related applications.\n",
    "\n",
    "[Learn more about Sentinel-2 here](https://www.usgs.gov/centers/eros/science/usgs-eros-archive-sentinel-2?qt-science_center_objects=0#qt-science_center_objects).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PpjeXFbAMRB"
   },
   "source": [
    "## Google Earth Engine\n",
    "In this section, we will demonstrate how to use Google Engine to download Sentinel-2 satellite imagery. Again, for simplicity, we will only download Sentinel-2 RGB bands - red (B4), green (B3), and blue (B2).\n",
    "\n",
    "In the following cell, we define a function to generate a Sentinel-2 image from Google Earth using the Python Earth Engine API. In order to minimize cloud cover, we chose to aggregate a collection of images over a period of time, as opposed to obtaining a single image on a given date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFlTm3WHaSPZ"
   },
   "outputs": [],
   "source": [
    "def generate_image(\n",
    "    region,\n",
    "    product='COPERNICUS/S2',\n",
    "    min_date='2018-01-01',\n",
    "    max_date='2020-01-01',\n",
    "    range_min=0,\n",
    "    range_max=2000,\n",
    "    cloud_pct=10\n",
    "):\n",
    "\n",
    "    \"\"\"Generates cloud-filtered, median-aggregated\n",
    "    Sentinel-2 image from Google Earth Engine using the\n",
    "    Pythin Earth Engine API.\n",
    "\n",
    "    Args:\n",
    "      region (ee.Geometry): The geometry of the area of interest to filter to.\n",
    "      product (str): Earth Engine asset ID\n",
    "        You can find the full list of ImageCollection IDs\n",
    "        at https://developers.google.com/earth-engine/datasets\n",
    "      min_date (str): Minimum date to acquire collection of satellite images\n",
    "      max_date (str): Maximum date to acquire collection of satellite images\n",
    "      range_min (int): Minimum value for visalization range\n",
    "      range_max (int): Maximum value for visualization range\n",
    "      cloud_pct (float): The cloud cover percent to filter by (default 10)\n",
    "\n",
    "    Returns:\n",
    "      ee.image.Image: Generated Sentinel-2 image clipped to the region of interest\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate median aggregated composite\n",
    "    image = ee.ImageCollection(product)\\\n",
    "        .filterBounds(region)\\\n",
    "        .filterDate(str(min_date), str(max_date))\\\n",
    "        .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", cloud_pct))\\\n",
    "        .median()\n",
    "\n",
    "    # Get RGB bands\n",
    "    image = image.visualize(bands=['B4', 'B3', 'B2'], min=range_min, max=range_max)\n",
    "    # Note that the max value of the RGB bands is set to 65535\n",
    "    # because the bands of Sentinel-2 are 16-bit integers\n",
    "    # with a full numerical range of [0, 65535] (max is 2^16 - 1);\n",
    "    # however, the actual values are much smaller than the max value.\n",
    "    # Source: https://stackoverflow.com/a/63912278/4777141\n",
    "\n",
    "    return image.clip(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i1asxmpJZk2"
   },
   "source": [
    "We generate and visualize the Sentinel-2 satellite image for the selected Uganda district. The satellite image is generated by getting the median of all Sentinel-2 images in 2020 with a cloud cover of less than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A4reLL02dp7",
    "outputId": "08bd93de-f5a0-426e-8367-d2396a447c49",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    }
   },
   "outputs": [],
   "source": [
    "# Get the shape geometry for the selected Uganda district\n",
    "region  = geoboundary.loc[geoboundary.shapeName == shape_name]\n",
    "centroid = region.iloc[0].geometry.centroid.coords[0]\n",
    "region = eec.gdfToFc(region)\n",
    "#geodataframe to featureCollection for EE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSk-pQTb-hx7"
   },
   "source": [
    "## Export Image to Local Gdrive\n",
    "In the following cell, we define a function to export our generated Sentinel-2 satellite image to our local Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0crFiRfEaOhe"
   },
   "outputs": [],
   "source": [
    "def export_image(image, filename, region, folder):\n",
    "    \"\"\"Export Image to Google Drive.\n",
    "\n",
    "    Args:\n",
    "      image (ee.image.Image): Generated Sentinel-2 image\n",
    "      filename (str): Name of image, without the file extension\n",
    "      geometry (ee.geometry.Geometry): The geometry of the area of\n",
    "        interest to filter to.\n",
    "      folder (str): The destination folder in your Google Drive.\n",
    "\n",
    "    Returns:\n",
    "      ee.batch.Task: A task instance\n",
    "    \"\"\"\n",
    "\n",
    "    print('Exporting to {}.tif ...'.format(filename))\n",
    "\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "      image=image,\n",
    "      driveFolder=folder,\n",
    "      scale=10,\n",
    "      region=region.geometry(),\n",
    "      description=filename,\n",
    "      fileFormat='GeoTIFF',\n",
    "      crs='EPSG:4326',\n",
    "      maxPixels=900000000\n",
    "    )\n",
    "    task.start()\n",
    "\n",
    "    return task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRsopDluL-HV"
   },
   "source": [
    "We can now proceed to download the image to our local Google Drive as a GeoTIFF.\n",
    "\n",
    "**Note**: Be careful about exporting large images as they can take a while to download and could eat up storage space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AF9_y1X7PaU",
    "outputId": "db1b7b6b-ab0b-405c-b5df-5408b595bf71",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "folder = 'Colab Notebooks' # Change this to your file destination folder in Google drive\n",
    "task = export_image(image, shape_name, region, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCzghtIeN4RC"
   },
   "source": [
    "You can repeatedly run `task.status()` to monitor the state of the task. After a while, the state should change from \"READY\" to \"RUNNING\" to \"COMPLETE\".\n",
    "\n",
    "Alternatively, you can go to https://code.earthengine.google.com/ to check the status of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwBUGymaN0iV",
    "outputId": "707682e8-f44f-4033-aaf5-7b2cc435b3e4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTfTOeimJeqJ"
   },
   "source": [
    "## Visualize Sentinel-2A Image\n",
    "\n",
    "Once the task status changes to \"COMPLETE\", check that the satellite image is in your google drive.\n",
    "\n",
    "In the following cell, we load and visualize the satellite raster image using the Rasterio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqiAA7gQITY9",
    "outputId": "85516e56-e548-452a-d017-0f16068379fa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    }
   },
   "outputs": [],
   "source": [
    "# Change this to your image file path\n",
    "cwd = './drive/My Drive/Colab Notebooks/'\n",
    "tif_file = os.path.join(cwd, '{}.tif'.format(shape_name))\n",
    "\n",
    "# Uncomment this to download the TIF file\n",
    "if not os.path.isfile(tif_file):\n",
    "  tif_file = '{}.tif'.format(shape_name)\n",
    "  !gdown \"12VJQBht4n544OXh4dmugqMESXXxRlBcU\"\n",
    "\n",
    "# Open image file using Rasterio\n",
    "image = rio.open(tif_file)\n",
    "boundary = geoboundary[geoboundary.shapeName == shape_name]\n",
    "\n",
    "# Plot image and corresponding boundary\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "boundary.plot(facecolor=\"none\", edgecolor='red', ax=ax)\n",
    "show(image, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB3cNEKYalCA"
   },
   "source": [
    "<a name=\"tiles\"></a>\n",
    "# Generate 64x64 px GeoJSON Tiles\n",
    "\n",
    "Recall that in previous tutorial, we trained a deep learning model on the [EuroSAT RGB dataset](), which consists of 64x64 pixel Sentinel-2 image patches. This means that we will also need to break down our huge Sentinel-2 image into smaller 64x64 px tiles.\n",
    "\n",
    "Let's start by creating a function that generates a grid of 64x64 px square polygons using [Rasterio Window utilities](https://rasterio.readthedocs.io/en/latest/api/rasterio.windows.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqbY3WyKaAqf"
   },
   "outputs": [],
   "source": [
    "def generate_tiles(image_file, output_file, area_str, size=64):\n",
    "    \"\"\"Generates 64 x 64 polygon tiles.\n",
    "\n",
    "    Args:\n",
    "      image_file (str): Image file path (.tif)\n",
    "      output_file (str): Output file path (.geojson)\n",
    "      area_str (str): Name of the region\n",
    "      size(int): Window size\n",
    "\n",
    "    Returns:\n",
    "      GeoPandas DataFrame: Contains 64 x 64 polygon tiles\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster image using rasterio\n",
    "    raster = rio.open(image_file)\n",
    "    width, height = raster.shape\n",
    "\n",
    "    # Create a dictionary which will contain our 64 x 64 px polygon tiles\n",
    "    # Later we'll convert this dict into a GeoPandas DataFrame.\n",
    "    geo_dict = { 'id' : [], 'geometry' : []}\n",
    "    index = 0\n",
    "\n",
    "    # Do a sliding window across the raster image\n",
    "    with tqdm(total=width*height) as pbar:\n",
    "      for w in range(0, width, size):\n",
    "          for h in range(0, height, size):\n",
    "              # Create a Window of your desired size\n",
    "              window = rio.windows.Window(h, w, size, size)\n",
    "              # Get the georeferenced window bounds\n",
    "              bbox = rio.windows.bounds(window, raster.transform)\n",
    "              # Create a shapely geometry from the bounding box\n",
    "              bbox = box(*bbox)\n",
    "\n",
    "              # Create a unique id for each geometry\n",
    "              uid = '{}-{}'.format(area_str.lower().replace(' ', '_'), index)\n",
    "\n",
    "              # Update dictionary\n",
    "              geo_dict['id'].append(uid)\n",
    "              geo_dict['geometry'].append(bbox)\n",
    "\n",
    "              index += 1\n",
    "              pbar.update(size*size)\n",
    "\n",
    "    # Cast dictionary as a GeoPandas DataFrame\n",
    "    results = gpd.GeoDataFrame(pd.DataFrame(geo_dict))\n",
    "    # Set CRS to EPSG:4326\n",
    "    results.crs = {'init' :'epsg:4326'}\n",
    "    # Save file as GeoJSON\n",
    "    results.to_file(output_file, driver=\"GeoJSON\")\n",
    "\n",
    "    raster.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FohLbxEWWQx"
   },
   "source": [
    "We can now create square polygons of size 64x64 px across the the selected Uganda district Sentinel-2 satellite image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8AthojMch2U",
    "outputId": "f2210767-09cf-4a04-f211-1fec0091c735",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247,
     "referenced_widgets": [
      "a2d14e8e45c84f6cbe367866578d5bbe",
      "8acfe33dc8034de0ba80903e51de80bb",
      "6fbb6f455265478b9b2778e226c5aadd",
      "062953fc7bd547b8b7126d0fa54470a6",
      "4c31ee21487d4d499912caf8a012d8e3",
      "35254bf0595d4dabac19409be381d712",
      "21124ada9f2e4c959df886523c5cf93e",
      "e48c7306feda42668b56ce717e0598f7",
      "ede4d37871474da7819c0db9a7e361ae",
      "2a8ef3f2551b40c692e2906c1b24f61e",
      "6f2c023eb99b40f18a0ace36c114bf3e"
     ]
    }
   },
   "outputs": [],
   "source": [
    "output_file = os.path.join(cwd, '{}.geojson'.format(shape_name))\n",
    "tiles = generate_tiles(tif_file, output_file, shape_name, size=64)\n",
    "\n",
    "# Uncomment this to download GeoJSON file\n",
    "#if not os.path.isfile(output_file):\n",
    "#  output_file = '{}.geojson'.format(shape_name)\n",
    "#  !gdown \"1h7L17F0SD1xuppWddqAVh64zxH7Cjf9p\"\n",
    "\n",
    "print('Data dimensions: {}'.format(tiles.shape))\n",
    "tiles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mk9nhx7fvD3"
   },
   "source": [
    "## Visualize 64x64 px Tiles\n",
    "\n",
    "Let's open the Sentinel-2 raster file using Rasterio and superimpose the 64x64px vector polygons as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDRoFns7WLQc",
    "outputId": "8d751279-b22d-45ab-bd63-de809a803839",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    }
   },
   "outputs": [],
   "source": [
    "image = rio.open(tif_file)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "tiles.plot(facecolor=\"none\", edgecolor='red', ax=ax)\n",
    "show(image, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uz4luVvSXIyi"
   },
   "source": [
    "Notice that the polygons are generated for empty (black) regions as well. Using our model to predict on blank regions seems computationally wasteful.\n",
    "\n",
    "Instead, we can get the intersection between:\n",
    "- the the selected Uganda district boundary polygon and\n",
    "- the 64 x 64 grid tiles.\n",
    "\n",
    "To do this, we use GeoPandas `.sjoin()` function. We set parameter `op='within'` to indicate that we only want the tiles that lie within the district boundary.\n",
    "\n",
    "[See  more information on GeoPandas sjoin operation here](https://geopandas.org/reference/geopandas.sjoin.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUUhdEnohXk8",
    "outputId": "5beb98f9-c96b-4a43-c533-32a034f7f226",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    }
   },
   "outputs": [],
   "source": [
    "image = rio.open(tif_file)\n",
    "\n",
    "# Geopandas sjoin function\n",
    "tiles = gpd.sjoin(tiles, boundary, op='within')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "tiles.plot(facecolor=\"none\", edgecolor='red', ax=ax)\n",
    "show(image, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5ZaGJq5Bm8H"
   },
   "source": [
    "## Visualize Single Cropped Image\n",
    "We can now crop our Sentinel-2 image using the generated grids.\n",
    "\n",
    "Here, we visualize the Sentinel-2 image cropped using the first tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B78Y6bhAf5R6",
    "outputId": "d9249cb2-be71-4f14-81a8-171fd49b4c24",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    }
   },
   "outputs": [],
   "source": [
    "def show_crop(image, shape, title=''):\n",
    "  \"\"\"Crops an image based on the polygon shape.\n",
    "  Reference: https://rasterio.readthedocs.io/en/latest/api/rasterio.mask.html#rasterio.mask.mask\n",
    "\n",
    "  Args:\n",
    "    image (str): Image file path (.tif)\n",
    "    shape (geometry): The tile with which to crop the image\n",
    "    title(str): Image title\n",
    "  \"\"\"\n",
    "\n",
    "  with rio.open(image) as src:\n",
    "      out_image, out_transform = rio.mask.mask(src, shape, crop=True)\n",
    "      # Crop out black (zero) border\n",
    "      _, x_nonzero, y_nonzero = np.nonzero(out_image)\n",
    "      out_image = out_image[\n",
    "        :,\n",
    "        np.min(x_nonzero):np.max(x_nonzero),\n",
    "        np.min(y_nonzero):np.max(y_nonzero)\n",
    "      ]\n",
    "      # Visualize image\n",
    "      show(out_image, title=title)\n",
    "\n",
    "show_crop(tif_file, [tiles.iloc[5]['geometry']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTGB6vd17ETz"
   },
   "source": [
    "<a name=\"lulc-maps\"></a>\n",
    "# Generate Land Use and Land Cover Map\n",
    "In this section, we will generate our land use and land cover classification map using the trained model from the previous tutorial. Recall that the EuroSAT dataset consists of 10 different LULC classes as listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH1EE4JAeVsK"
   },
   "outputs": [],
   "source": [
    "# LULC Classes\n",
    "classes = [\n",
    "  'National boundary',\n",
    "  'District boundary',\n",
    "  'Agricultural land - commercial',\n",
    "  'Agricultural land - irrigated',\n",
    "  'Agricultural land - subsistence',\n",
    "  'Bushlands - high livestock density',\n",
    "  'Bushlands - low livestock density',\n",
    "  'Bushlands - protected',\n",
    "  'Bushlands - unprotected',\n",
    "  'Grasslands - high livestock density',\n",
    "  'Grasslands - low livestock density',\n",
    "  'Grasslands - moderate livestock density',\n",
    "  'Grasslands - protected',\n",
    "  'Grasslands - unprotected',\n",
    "  'Impediments - protected',\n",
    "  'Impediments - unprotected',\n",
    "  'Open water - protected',\n",
    "  'Tropical high Forest - with livestock activities',\n",
    "  'Tropical high Forests - protected',\n",
    "  'Tropical high forest (encroachment) - subsistence',\n",
    "  'Tropical high forest - tree plantations',\n",
    "  'Urban - settlement',\n",
    "  'Wetlands - protected',\n",
    "  'Wetlands - with crop farmland activities',\n",
    "  'Wetlands - with livestock activities',\n",
    "  'Woodland/forest - high livestock density',\n",
    "  'Woodland/forest - low livestock density',\n",
    "  'Woodland/forest - protected',\n",
    "  'Woodland/forest - unprotected',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Azure OpenAI Labeling & QA/QC\n",
    "If you are using Azure OpenAI to assist labeling or QA/QC, load tiles and predictions into a review pipeline to validate or refine classes before generating the final vector map.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Placeholder: integrate Azure OpenAI labeling/QAQC here.\n",
    "# Example: load a review table, call your labeling service, then update tiles['label'] accordingly.\n",
    "# tiles['label'] = tiles['label'].apply(your_azure_openai_reviewer)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW9uZiiZBxX3"
   },
   "source": [
    "## Load Model trained on EuroSAT\n",
    "First, load your trained model.\n",
    "\n",
    "In case you missed Part 1 of the tutorial, you can also uncomment the code below to download the trained model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaU__HOFBvNJ",
    "outputId": "0558256a-554a-4dd2-bf8f-8d2cd2cd0ab5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_file = cwd+'/models/best_model.pth'\n",
    "\n",
    "# Uncomment this to download the model file\n",
    "if not os.path.isfile(model_file):\n",
    "  model_file = 'best_model.pth'\n",
    "  !gdown \"13AFOESwxKmexCoOeAbPSX_wr-hGOb9YY\"\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print('Model file {} successfully loaded.'.format(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P2gEnIHqBY0"
   },
   "source": [
    "Remember in the previous tutorial that we applied a set of data transformations to our test set. Before we run our new images through the model, we'll need to apply these same transformation to our new data as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZfuGxmIp6Ra"
   },
   "outputs": [],
   "source": [
    "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHU3Wn7pIRr"
   },
   "source": [
    "## Model Prediction & LULC Map Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OY8nng6bk46"
   },
   "source": [
    "Next, let's define a function that:\n",
    "1. Crops the source image using the 64x64 tile geometry\n",
    "2. Generates a prediction for the cropped image using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u05VmCkdGv8"
   },
   "outputs": [],
   "source": [
    "def predict_crop(image, shape, classes, model, show=False):\n",
    "    \"\"\"Generates model prediction using trained model\n",
    "\n",
    "    Args:\n",
    "      image (str): Image file path (.tiff)\n",
    "      shape (geometry): The tile with which to crop the image\n",
    "      classes (list): List of LULC classes\n",
    "\n",
    "    Return\n",
    "      str: Predicted label\n",
    "    \"\"\"\n",
    "\n",
    "    with rio.open(image) as src:\n",
    "        # Crop source image using polygon shape\n",
    "        # See more information here:\n",
    "        # https://rasterio.readthedocs.io/en/latest/api/rasterio.mask.html#rasterio.mask.mask\n",
    "        out_image, out_transform = rio.mask.mask(src, shape, crop=True)\n",
    "        # Crop out black (zero) border\n",
    "        _, x_nonzero, y_nonzero = np.nonzero(out_image)\n",
    "        out_image = out_image[\n",
    "          :,\n",
    "          np.min(x_nonzero):np.max(x_nonzero),\n",
    "          np.min(y_nonzero):np.max(y_nonzero)\n",
    "        ]\n",
    "\n",
    "        # Get the metadata of the source image and update it\n",
    "        # with the width, height, and transform of the cropped image\n",
    "        out_meta = src.meta\n",
    "        out_meta.update({\n",
    "              \"driver\": \"GTiff\",\n",
    "              \"height\": out_image.shape[1],\n",
    "              \"width\": out_image.shape[2],\n",
    "              \"transform\": out_transform\n",
    "        })\n",
    "\n",
    "        # Save the cropped image as a temporary TIFF file.\n",
    "        temp_tif = 'temp.tif'\n",
    "        with rio.open(temp_tif, \"w\", **out_meta) as dest:\n",
    "          dest.write(out_image)\n",
    "\n",
    "        # Open the cropped image and generated prediction\n",
    "        # using the trained Pytorch model\n",
    "        image = Image.open(temp_tif)\n",
    "        input = transform(image)\n",
    "        output = model(input.unsqueeze(0))\n",
    "        _, pred = torch.max(output, 1)\n",
    "        label = str(classes[int(pred[0])])\n",
    "\n",
    "        if show:\n",
    "          out_image.show(title=label)\n",
    "\n",
    "        return label\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAOvCSm4efbZ"
   },
   "source": [
    "Let's iterate over every 64x64 px tile and generate model predictions for the corresponding cropped image. Note that we are overwriting each temporary TIFF file to save storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVIyLD_kdKTa",
    "outputId": "78acfaeb-039e-47ba-a0b0-a44fda89a561",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455,
     "referenced_widgets": [
      "035ad63e455c4026aacca2e402538b71",
      "9532a9839ba04ae8b854f7e1d3fdc110",
      "9dbbdf9163304fcf89e12debb648f28a",
      "3088325bd0a94799ace6218b364cd52b",
      "5f39da72707c4607a6230bff75c4683f",
      "26764daea26c4a1590e88253aaa09adc",
      "35e7054fab4147af8eccfbb44b76d01d",
      "0d110c7f29464c7a81b3a574ef803e80",
      "8addad95333e4800b85bb9fb4b4ff9f3",
      "97fc0e03d3bd428a9c6f2197527b8009",
      "872617f1ea154997908f7dd8545453a6"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Commence model prediction\n",
    "labels = [] # Store predictions\n",
    "for index in tqdm(range(len(tiles)), total=len(tiles)):\n",
    "  label = predict_crop(tif_file, [tiles.iloc[index]['geometry']], classes, model)\n",
    "  labels.append(label)\n",
    "tiles['pred'] = labels\n",
    "\n",
    "# Save predictions\n",
    "filepath = os.path.join(cwd, \"{}_preds.geojson\".format(shape_name))\n",
    "tiles.to_file(filepath, driver=\"GeoJSON\")\n",
    "\n",
    "tiles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DOj0AM_uWA3"
   },
   "source": [
    "## Visualize Interactive LULC Map\n",
    "Lastly, we show you how to generate an interactive LULC map using Folium.\n",
    "\n",
    "Let's start by loading the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tsYl4tT-LjD",
    "outputId": "bbc6a28f-ddb3-4234-b7aa-afa6b4696d5e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    }
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(cwd, \"{}_preds.geojson\".format(shape_name))\n",
    "\n",
    "# Uncomment this to download the model predictions\n",
    "if not os.path.isfile(filepath):\n",
    "  filepath = \"{}_preds.geojson\".format(shape_name)\n",
    "  !gdown \"1LN4efjd3WPGB1TtNiaHcRbFyBzbFY52A\"\n",
    "\n",
    "tiles = gpd.read_file(filepath)\n",
    "tiles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coGTqvMa-MKb"
   },
   "source": [
    "We then map each label to a corresponding color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANGUEZdjqLCc",
    "outputId": "d375048b-8cc4-4146-8ddb-b6b011e02781",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    }
   },
   "outputs": [],
   "source": [
    "# We map each class to a corresponding color\n",
    "colors = {\n",
    "  'National boundary' : 'black',\n",
    "  'District boundary' : 'dimgray',\n",
    "  'Agricultural land - commercial' : 'orange',\n",
    "  'Agricultural land - irrigated' : 'gold',\n",
    "  'Agricultural land - subsistence' : 'khaki',\n",
    "  'Bushlands - high livestock density' : 'saddlebrown',\n",
    "  'Bushlands - low livestock density' : 'peru',\n",
    "  'Bushlands - protected' : 'darkolivegreen',\n",
    "  'Bushlands - unprotected' : 'olivedrab',\n",
    "  'Grasslands - high livestock density' : 'yellowgreen',\n",
    "  'Grasslands - low livestock density' : 'lightgreen',\n",
    "  'Grasslands - moderate livestock density' : 'palegreen',\n",
    "  'Grasslands - protected' : 'mediumseagreen',\n",
    "  'Grasslands - unprotected' : 'seagreen',\n",
    "  'Impediments - protected' : 'darkslategray',\n",
    "  'Impediments - unprotected' : 'lightgray',\n",
    "  'Open water - protected' : 'deepskyblue',\n",
    "  'Tropical high Forest - with livestock activities' : 'forestgreen',\n",
    "  'Tropical high Forests - protected' : 'darkgreen',\n",
    "  'Tropical high forest (encroachment) - subsistence' : 'limegreen',\n",
    "  'Tropical high forest - tree plantations' : 'teal',\n",
    "  'Urban - settlement' : 'tomato',\n",
    "  'Wetlands - protected' : 'mediumaquamarine',\n",
    "  'Wetlands - with crop farmland activities' : 'turquoise',\n",
    "  'Wetlands - with livestock activities' : 'cadetblue',\n",
    "  'Woodland/forest - high livestock density' : 'purple',\n",
    "  'Woodland/forest - low livestock density' : 'magenta',\n",
    "  'Woodland/forest - protected' : 'deeppink',\n",
    "  'Woodland/forest - unprotected' : 'hotpink',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJHwdIclqTj1"
   },
   "source": [
    "Note that you can toggle the map on/off using the upper right controls."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Instantiate map centered on the centroid\n",
    "map = folium.Map(location=[centroid[1], centroid[0]], zoom_start=10)\n",
    "\n",
    "# Add Google Satellite basemap\n",
    "folium.TileLayer(\n",
    "      tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "      attr = 'Google',\n",
    "      name = 'Google Satellite',\n",
    "      overlay = True,\n",
    "      control = True\n",
    ").add_to(map)\n",
    "\n",
    "# Add LULC Map with legend\n",
    "legend_txt = '<span style=\"color: {col};\">{txt}</span>'\n",
    "for label, color in colors.items():\n",
    "\n",
    "  # Specify the legend color\n",
    "  name = legend_txt.format(txt=label, col=color)\n",
    "  feat_group = folium.FeatureGroup(name=name)\n",
    "\n",
    "  # Add GeoJSON to feature group\n",
    "  subtiles = tiles[tiles.pred==label]\n",
    "  if len(subtiles) > 0:\n",
    "    folium.GeoJson(\n",
    "        subtiles,\n",
    "        style_function=lambda feature: {\n",
    "          'fillColor': feature['properties']['color'],\n",
    "          'color': 'black',\n",
    "          'weight': 1,\n",
    "          'fillOpacity': 0.5,\n",
    "        },\n",
    "        name='LULC Map'\n",
    "    ).add_to(feat_group)\n",
    "    map.add_child(feat_group)\n",
    "\n",
    "folium.LayerControl().add_to(map)\n",
    "map"
   ],
   "metadata": {
    "id": "leehlKA5KGyo",
    "outputId": "b9ba42c8-85cc-492a-bf3b-08592d2c0dde",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keQ6ad_RnY6j"
   },
   "source": [
    "# Conclusion\n",
    "Congrats on making it to the end! To recap, in this tutorial, you've learned how to download a Sentinel-2 satellite image for a region of interest using Google Earth Engine and apply a trained CNN model to generate a land use and land cover map. As an exercise, try applying the model to another region, e.g. your home country. How well does the model perform in this new geography?\n",
    "\n",
    "## Data Limitations\n",
    "If you've tried applying the model to another region of the world, you'll find that the model does not perform as well for certain areas. We note some of the limitations of the EuroSAT dataset as follows:\n",
    "- **Limited scene categories.** The 10 land cover classes in the EuroSAT dataset are not representative of the complex content of remote sensing data. These class labels are not mutually disjoint (e.g. an image can contain both a highway and a residential area) and their union does not cover real-world distribution (e.g. certain land cover types like desert land and aquaculture are not present in the dataset).\n",
    "- **Limited model transferrability.** Like many existing remote sensing datasets, EuroSAT, which consists of satellite images distributed across Europe, suffers from limited geographic coverage which restricts the model's generalizability to other regions of the world. Thus, collaboration with diverse research institutions and stronger data sharing efforts are necessary to improve the global coverage of annotated remote sensing datasets.\n",
    "\n",
    "## Climate-related Applications\n",
    "- **Land use and land cover change detection.** Given that Sentinel-2 will continue to collect RS data for the next several decades, one promising next step is to use the trained model to observe and detect changes in land cover. [MapBiomas](https://plataforma.brasil.mapbiomas.org/), for example, is a platform that visualizes LULC changes in Brazil over a long period of time. This can be particularly useful for urban planning, environmental monitoring, and nature protection. Deforestation, for example, contributes significantly to climate change; monitoring changes in forest cover and identifying drivers of forest loss can be useful for forest conservation and restoration efforts.\n",
    "- **Analyzing carbon emissions from land use change.**  Analyzing land use category conversion in conjunction with changes in soil carbon storage can help quantify the contribution of land use change and land management to total carbon emissions, as demonstrated in this [2016 study by Lai et al](https://advances.sciencemag.org/content/2/11/e1601063). The study found that land use change - particularly urbanization, which has led to rapid expansion of built-up areas and massive loss of terrestrial carbon storage - has resulted in large carbon emissions in China. This can significantly undermine carbon emission reduction targets unless appropriate measures are taken to control urbanization and improve land management.\n",
    "- **Vulnerability assessment of different land cover types.** Overlaying land cover maps with various geospatial hazard maps (e.g. hurricane paths, earthquake faults, and flood maps) and climate projection maps can be useful for assessing vulnerability of certain land cover types, such as settlements and agricultural land, to different risks. When shared with humanitarian organizations and government agencies, these maps have the potential to support disaster risk reduction planning as well as long-term climate mitigation and adaptation efforts.\n",
    "\n",
    "## Other Remote Sensing Datasets\n",
    "- So2Sat LCZ42: A benchmark dataset for global local climate zones classification  ([data](https://mediatum.ub.tum.de/1483140), [paper](https://arxiv.org/pdf/1912.12171.pdf))\n",
    "- RESISC45: High resolution remote sensing scene classification dataset( [data](https://www.tensorflow.org/datasets/catalog/resisc45), [paper](https://arxiv.org/abs/1703.00121))\n",
    "- BigEarthNet: Large-Scale Sentinel-2 Benchmark ([data](http://bigearth.net/), [paper](https://arxiv.org/abs/1902.06148))\n",
    "\n",
    "[Check out this Github repository](https://github.com/chrieke/awesome-satellite-imagery-datasets) for a more comprehensive collection of satellite imagery datasets.\n",
    "\n",
    "## Next Steps\n",
    "**Interested in learning more about climate change and machine learning?**\n",
    "\n",
    "We encourage you to check out [our paper](), which provides a detailed guide of ways machine learning can be used to tackle climate change. Please also feel free to check out our [wiki]() and [tutorials]() on our website. We also encourage you to join the conversations on our [discussion forum](), submit to our [workshops](), attend our [events]() and [workshops](), and of course, sign up for our [newletter]()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZsuntDvXoaX"
   },
   "source": [
    "# Feedback\n",
    "Have any comments/suggestions/feedback? Interested in collaborating?\n",
    "\n",
    "Contact us at:\n",
    "*   ankur.mahesh@berkeley.edu\n",
    "*   issatingzon@climatechange.ai\n",
    "*   milojevicdupontn@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILNfNgDK4XfK"
   },
   "source": [
    "# References\n",
    "- Coordinate Reference Systems \u2013 Introduction to Geospatial Concepts. (n.d.). Data Carpentry - Introduction to Geospatial Concepts. Retrieved February 14, 2021, from https://datacarpentry.org/organization-geospatial/03-crs/\n",
    "- USGS EROS Archive - Sentinel-2. (n.d.). USGS. Retrieved February 14, 2021, from https://www.usgs.gov/centers/eros/science/usgs-eros-archive-sentinel-2?qt-science_center_objects=0#qt-science_center_objects\n",
    "- Long, Yang, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li. \u201cDIRS: On creating benchmark datasets for remote sensing image interpretation.\u201d arXiv preprint arXiv:2006.12485 (2020). https://arxiv.org/pdf/1912.12171.pdf\n",
    "- Zhu, Xiao Xiang, et al. \u201cSo2Sat LCZ42: A benchmark dataset for global local climate zones classification.\u201d arXiv preprint arXiv:1912.12171 (2019). https://arxiv.org/pdf/1912.12171.pdf\n",
    "- Sumbul, Gencer, et al. \"Bigearthnet: A large-scale benchmark archive for remote sensing image understanding.\" IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2019.\n",
    "- Lai, Li, et al. \"Carbon emissions from land-use change and management in China between 1990 and 2010.\" Science Advances 2.11 (2016): e1601063."
   ]
  }
 ]
}